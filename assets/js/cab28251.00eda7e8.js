"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[1947],{28453:(e,t,n)=>{n.d(t,{R:()=>a,x:()=>i});var s=n(96540);const o={},r=s.createContext(o);function a(e){const t=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function i(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),s.createElement(r.Provider,{value:t},e.children)}},28821:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>i,default:()=>f,frontMatter:()=>a,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"gists/Python/Vannilla_Transformers","title":"Vannilla Transformers","description":"The encoder stack consists of layers each of which encodes the input, normalizes it, carries out Multihead attention (to identify contextual dependencies), and then passes these vectors through a fully connected network until the output of the final ecoder layer is concatenated to the decoder input. The decoder stack consists of another set of feed-forward networks responsible for next-token prediction.","source":"@site/docs/gists/Python/Vannilla_Transformers.md","sourceDirName":"gists/Python","slug":"/gists/Python/Vannilla_Transformers","permalink":"/docs/gists/Python/Vannilla_Transformers","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"Vannilla_Transformers","title":"Vannilla Transformers","sidebar_label":"Vannilla Transformers"},"sidebar":"tutorialSidebar","previous":{"title":"Deep Learning Frameworks","permalink":"/docs/gists/Python/Deep_Learning_Frameworks"},"next":{"title":"GPU Testing Code TF","permalink":"/docs/gists/Python/gpu_testing_code_tf"}}');var o=n(74848),r=n(28453);const a={id:"Vannilla_Transformers",title:"Vannilla Transformers",sidebar_label:"Vannilla Transformers"},i=void 0,c={},l=[];function d(e){const t={p:"p",...(0,r.R)(),...e.components};return(0,o.jsx)(t.p,{children:"The encoder stack consists of layers each of which encodes the input, normalizes it, carries out Multihead attention (to identify contextual dependencies), and then passes these vectors through a fully connected network until the output of the final ecoder layer is concatenated to the decoder input. The decoder stack consists of another set of feed-forward networks responsible for next-token prediction."})}function f(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}}}]);