"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[8130],{7735:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"nvidia-drivers","metadata":{"permalink":"/blog/nvidia-drivers","source":"@site/blog/2025-06-22-Installing_Nvidia_Drivers/story.md","title":"Setting up Nvidia Drivers for GPU access on Ubuntu","description":"Read on Medium","date":"2025-06-22T00:00:00.000Z","tags":[{"inline":false,"label":"NVIDIA","permalink":"/blog/tags/nvidia","description":"NVIDIA tag description"},{"inline":false,"label":"Drivers","permalink":"/blog/tags/drivers","description":"Drivers tag description"},{"inline":false,"label":"CUDA","permalink":"/blog/tags/cuda","description":"CUDA tag description"},{"inline":false,"label":"Ubuntu","permalink":"/blog/tags/ubuntu","description":"Ubuntu tag description"},{"inline":false,"label":"Ollama","permalink":"/blog/tags/ollama","description":"Ollama tag description"},{"inline":false,"label":"GPU","permalink":"/blog/tags/gpu","description":"GPU tag description"}],"readingTime":5.01,"hasTruncateMarker":true,"authors":[{"name":"Ajay T Shaju","title":"AI & DS Engineer","url":"https://004ajay.github.io/","page":{"permalink":"/blog/authors/ajay"},"socials":{"linkedin":"https://www.linkedin.com/in/ajay-t-shaju/","github":"https://github.com/004ajay","medium":"https://medium.com/@ajaytshaju","x":"https://x.com/004ajayt"},"imageURL":"https://github.com/004ajay.png","key":"ajay"}],"frontMatter":{"slug":"nvidia-drivers","title":"Setting up Nvidia Drivers for GPU access on Ubuntu","authors":["ajay"],"tags":["nvidia","drivers","cuda","ubuntu","ollama","gpu"]},"unlisted":false,"nextItem":{"title":"Bash to Fish: A Terminal Change","permalink":"/blog/bash-to-fish"}},"content":"Read on [Medium](https://medium.com/@ajaytshaju)\\r\\n\\r\\nAfter upgrading my Ubuntu from 18 to 22, I was left with no Nvidia drivers on my computer. When I type `nvidia-smi` and `nvcc -v` there was no good output (see the image below). So, I planned to share my method for setting up GPU drivers on my computer with the world. If your situation is the same, then follow along.\\r\\n\\r\\n\x3c!-- truncate --\x3e\\r\\n\\r\\n![No good output for nvidia-smi and nvcc -v](nvidia-smi%20and%20nvcc-v%20no%20output.png)\\r\\n\\r\\nNo good output for nvidia-smi and nvcc -v\\r\\n\\r\\nBefore starting, I will give a short lesson on drivers\\r\\n\\r\\n> Drivers allow OS \u2194 hardware communication. NVIDIA drivers are essential to communicate with your GPU, for graphics and compute functionality. \\r\\n\\r\\nDrivers are software that act as translators between your OS and the hardware (like the GPU, printer, or keyboard). NVIDIA drivers are crucial to communicate correctly with NVIDIA GPUs. `Nvidia Graphics Drivers` for rendering visuals in games and videos, and `Compute Unified Device Architecture (CUDA) Drivers` for GPU-accelerated computations like deep learning and scientific computing.\\r\\n\\r\\nNow let\'s start with\\r\\n\\r\\n## The real processes\\r\\n\\r\\n### Check if your system detects the GPU\\r\\n\\r\\n`lspci | grep -i nvidia`\\r\\n\\r\\nThis will check for the presence of an NVIDIA GPU in your system. If this command doesn\'t give a good output like in the image below, then you can stop here, because your system doesn\'t have an NVIDIA GPU or it is not detecting the GPU.\\r\\n\\r\\n![My system is detecting the Nvidia GPU](lspci%20output.png)\\r\\n\\r\\nMy system is detecting the Nvidia GPU\\r\\n\\r\\n### Install Drivers\\r\\n\\r\\nIf the above commands work, then try `nvidia-smi`, you might get a list of drivers as the output of nvidia-smi (see the image above). Normally we need to select the best driver, but sometimes we may not be up to date with the new driver details, so we can offload the driver version decision to our system by giving the command `ubuntu-drivers devices` this will show a list of available and compatible drivers for our GPU, then we will auto install the best driver using `sudo ubuntu-drivers autoinstall` this will show long commands running and installing, don\'t worry it is installing the best driver. On finishing, just give a good `sudo reboot` (restart) to your system, so the system can understand the new changes.\\r\\n\\r\\n> In between let me explain what is happening - When you install drivers using `sudo ubuntu-drivers autoinstall`, you\'re installing some files that allows Linux to detect and communicate with your GPU, this will enable `nvidia-smi` to show a box output. If you want to use GPU-accelerated apps like Blender, video editors, or run ML/DL programs (eg, inference using PyTorch or TensorFlow), then installing the driver is enough. But if you want to go further into ML/DL development and LLM finetuning which uses libraries like xFormers, bitsandbytes, and triton then you need to do some more steps, i.e installing `nvidia-cuda-toolkit`.\\r\\n\\r\\n\\r\\n### Checking nvidia-smi; Install nvidia-cuda-toolkit\\r\\n\\r\\n_**Note**: Read this section fully before running any commands_\\r\\n\\r\\nAfter reboot get into the terminal and quickly type `nvidia-smi` and you will get a box like output with information such as your GPU name, power usage, memory usage, CUDA version, processes utilizing gpu currently etc...but when typing `nvcc -v` there won\'t be any output and it will ask you to install nvidia-cuda-toolkit, and we need to do `sudo apt install nvidia-cuda-toolkit` which may take some time. This is linked with the ubuntu version you have, for Ubuntu 22.04, CUDA 11.7 or later may be installed, this is easy when you need to install cuda-toolkit for a usecase which requires any version above 11.7 (here). But If you want a specific version, like CUDA Toolkit 12.8, you need to install it manually from [this](https://developer.nvidia.com/cuda-toolkit-archive) website by selecting the version you need, giving info such as OS, Architecture, Distribution, Version, Installer Type and finally following the commands displayed.\\r\\n\\r\\nOn finishing, check the output of `nvcc -v` to confirm it has installed the toolkit.\\r\\n\\r\\n## PyTorch Installation (Optional)\\r\\n\\r\\n### Start Locally with PyTorch\\r\\n\\r\\nI need to install PyTorch for some coding purposes. I will be doing it in a conda environment (if you\'re not familiar with conda, you can use Python Virtual Environments, or you can follow this [conda website](https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html) to install conda, create and activate a new environment) and install the latest PyTorch that is compatible with your CUDA version - see their [Start Locally Website](https://pytorch.org/get-started/locally/), give details like your OS, package, language, compute platform and copy the final command and run on your virtual environment to install Pytorch, this will take some time based on your internet speed.\\r\\n\\r\\n### Checking if torch can access our GPU\\r\\n\\r\\nEnter Python IDLE on the terminal and run `import torch; torch.cuda.is_available()` - if it returns `True`, then your installation is successful. Otherwise, there is a problem with any commands or installations. If you have noticed any warning, red command, etc...then debug on that.\\r\\n\\r\\n![Checking if torch can access our GPU, on Python IDLE](Python%20IDLE%20for%20torch.png)\\r\\n\\r\\nChecking if torch can access our GPU, on Python IDLE\\r\\n\\r\\n## Test your installation\\r\\n\\r\\n### Setting up a local LLM\\r\\n\\r\\nIf everything is fine, we can try setting up Ollama ([Website](https://ollama.com/) | [GitHub](http://github.com/ollama/ollama)) and test whether an LLM is able to use our consumer GPU to predict tokens.\\r\\n\\r\\nInstall ollama using `curl -fsSL https://ollama.com/install.sh | sh` wait for it to finish and, finally, you can see a message from Ollama `NVIDIA GPU installed` and you can run a small LLM like llama or mistral (ollama run mistral) and test whether they\'re utilising our GPU by checking `nvtop` or see if the LLM is generating the text fast.\\r\\n\\r\\n![Ollama detects our GPU](Ollama%20Installation.png)\\r\\n\\r\\nOllama detects our GPU\\r\\n\\r\\nSo that\'s it, now you will have GPU acceleration for your tasks. Good Luck!"},{"id":"bash-to-fish","metadata":{"permalink":"/blog/bash-to-fish","source":"@site/blog/2025-05-04-Bash_to_Fish_A_Terminal_Change/story.md","title":"Bash to Fish: A Terminal Change","description":"Read on Medium","date":"2025-05-04T00:00:00.000Z","tags":[{"inline":false,"label":"Bash","permalink":"/blog/tags/bash","description":"Bash Shell"},{"inline":false,"label":"Fish","permalink":"/blog/tags/fish","description":"Fish Shell"}],"readingTime":10.06,"hasTruncateMarker":true,"authors":[{"name":"Ajay T Shaju","title":"AI & DS Engineer","url":"https://004ajay.github.io/","page":{"permalink":"/blog/authors/ajay"},"socials":{"linkedin":"https://www.linkedin.com/in/ajay-t-shaju/","github":"https://github.com/004ajay","medium":"https://medium.com/@ajaytshaju","x":"https://x.com/004ajayt"},"imageURL":"https://github.com/004ajay.png","key":"ajay"}],"frontMatter":{"slug":"bash-to-fish","title":"Bash to Fish: A Terminal Change","authors":["ajay"],"tags":["bash","fish"]},"unlisted":false,"prevItem":{"title":"Setting up Nvidia Drivers for GPU access on Ubuntu","permalink":"/blog/nvidia-drivers"},"nextItem":{"title":"How AI can be Dangerous to YouTubers but Helpful to Viewers","permalink":"/blog/ai-dangerous-to-youtubers"}},"content":"Read on [Medium](https://medium.com/@ajaytshaju/bash-to-fish-a-terminal-change-775e577608cc)\\n\\nI found the name **Fish Terminal** funny the first time I heard it. When I came to know about the full form of ***Fish, Friendly Interactive SHell***, I felt it was a good decision of [fish maintainers](https://github.com/fish-shell/fish-shell) putting the name as Fish, and the full form carries a heavy meaning too. I love the names of open source technologies, especially the acronyms they use \u2014 terminal names like Bash (Bourne Again SHell), Fish, Zsh (Z Shell) are very appropriate to them.\\n\\n\x3c!-- truncate --\x3e\\n\\n![Fish Image](fish.webp)\\nPhoto by [Claudio Guglieri](https://unsplash.com/@claudioguglieri?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)\\n\\nWhether it is the workplace or home, I use Linux a lot, and the terminal is the first thing I open and the last to close. Whether it\u2019s managing conda environments, SSH to remote servers, or running Python scripts\u2014 I live inside the terminal. For years, I used bash by default. Not because I loved it, but it comes with Ubuntu.\\n\\nLast month, until a workday\u2019s afternoon I typed many things like making activating and deleting many conda envs, scripting running python codes of varying lengths, ssh to different servers and setting up some docker containers and other services, after all the long typings my knuckles are so sore that I cannot fold them easily to hold the spoon to eat something, so as a first move to resolve this, my manager bought a new Logitech wireless keyboard for me, and it solved 50% of typing issues. But what about the remaining 50%?, That was filled with me writing repetitive commands of conda, long commands for Docker, and making Python scripts; frustrated and tired of this, I searched for a good terminal. Why? because \u201cI live in the terminal\u201d and found [Fish Shell](https://fishshell.com/). This post is all about my experience with fish shell, what things got easier, what improved, and how I use it, etc...\\n\\n![hand pain image](hand_pain.webp)\\nFinger pain due to typing, photo from [this](https://www.jonathanshultsmd.com/blog/can-typing-cause-carpal-tunnel-syndrome) website\\n\\n### Why I looked beyond bash\\n\\nBash is stable and battle-tested, no doubt. But it also feels stuck in time. It doesn\u2019t help you much. You have to remember everything. Tab completion is limited, no autosuggestions, no visual hints, and nothing that tells you you\u2019re typing the wrong path or missing a command. I started noticing how much time I was spending retyping things, checking history, running `ssh` manually to the same servers, or typing `conda activate some_env_name` again and again.\\nGetting started with Fish Shell (without extra effort)\\n\\nI installed Fish through Bash with some simple commands\\n\\n    sudo apt-get update && sudo apt-get upgrade\\n\\n    sudo apt-get install fish\\n\\nFish just worked out of the box. No need for any config files, setting paths, or anything. Just type `fish` on your bash to get started with fish shell.\\n\\n![Fish shell on first launch](fish_shell_on_first_launch.webp)\\nFish shell on first launch\\n\\nNow, instead of going through the features of the fish terminal, I\'m jumping to the customization options \u2014 I changed the theme, prompt style, and default greeting message to `Welcome AJAY!`, but only for a week.\\n\\nTo customize Fish shell, just type `fish_config` on the fish terminal and it will automatically display a browser window having all the configurations that can be done on the fish terminal.\\n\\n![fish_config browser UI](fish_config_browser_UI.webp)\\nfish_config browser UI\\n\\nBut in my home computer, when I type fish_config, it tries to open a temporary HTML file that doesn\u2019t open in the browser, but don\u2019t worry, just go to the same terminal and you can see a localhost link, ctrl + right click that link to get it opened on your browser.\\n\\n![Fish Config](Fish_Config.webp)\\nFish Config\\n\\nFrom this browser page, you can change `terminal color` , `prompt style`(_ajay@ajay_ ~>, in the first fish shell image) and see different fish functions and key bindings. Just change to whatever setting you like, and press `set prompt/color/theme`, close the browser tab, come back to the terminal, and hit the Enter key to stop configuring and saving the changes.\\n\\n#### Changing the greeting message\\n\\nThe greeting message seems to be very welcoming when we use the terminal once or twice a week, but it may not be when you use it 24/7. So, to erase the greeting message type `set fish_greeting` and hit Enter.\\n\\n![Turn off fish greeting](Turn_off_fish_greeting.webp)\\nTurn off fish greeting\\n\\nNow `clear` the terminal to see a terminal without a greeting message. But this is temporary, in the next session of the terminal, the greeting message will appear again.\\n\\n![terminal without greeting message](terminal_without_greeting_message.webp)\\nTerminal without greeting message\\n\\nYou can permanently erase the greeting message/put a custom greeting message in the terminal by editing the `fish_greeting.fish` file located at `/usr/share/fish/function/fish_greeting.fish`.\\n\\n![Edit fish_greeting function for displaying custom greeting message](Edit_fish_greeting_function_for_displaying_custom_greeting_message.webp)\\nEdit fish_greeting function for displaying custom greeting message\\n\\n![My custom greeting message](My_custom_greeting_message.webp)\\nMy custom greeting message\\n\\nYou can see the function definition while opening `fish_config` on browser, it can be used to find the location of that script too (first line in red color).\\n\\n![Fish function in functions tab on fish_config](Fish_function_in_functions_tab_on_fish_config.webp)\\nFish function in functions tab on fish_config\\n\\n### Features of Fish Terminal\\n\\n* **Autosuggestions**: Fish suggests commands based on your command history, which you can see using `history` command.\\n\\n![Autosuggestion in Fish Terminal](Autosuggestion_in_Fish_Terminal.gif)\\nAutosuggestion in Fish Terminal\\n\\nYou can either press `right arrow` or `Ctrl+F` to accept the auto suggestions. `Tab` press is similar to bash, if there is only one command as result, then tab will complete it, else it will display all the possible commands.\\n\\n![Autosuggestion and tab-press output1](Autosuggestion_and_tab-press_output1.webp)\\n![Autosuggestion and tab-press output2](Autosuggestion_and_tab-press_output2.webp)\\nAutosuggestion and tab-press output\\n\\n* **Syntax highlighting**: Wrong commands turn red. You fix them before running. In the GIF below, when I type `nvidia-si` the command got a red color, which means I typed the wrong command.\\n\\n![Syntax highlighting in Fish](Syntax_highlighting_in_Fish.gif)\\nSyntax highlighting in Fish\\n\\n* **Abbreviations**: You can set custom acronyms for long commands, and they will be expanded when you press the Spacebar or Enter.\\n\\n![Abbreviation in Fish](Abbreviation_in_Fish.gif)\\nAbbreviation in Fish\\n\\nTyping `ca nlp` becomes `conda activate nlp` in the terminal before hitting Enter. Likewise, you can make as many as you want.\\n\\n![List of abbreviations I am using](List_of_abbreviations_I_am_using.webp)\\nList of abbreviations I am using\\n\\nBy seeing the above list, the first thing you might think would be \u2014 how is this guy able to remember these many short forms, when learning these commands itself is a headache.\\n\\nThe answer is that I don\u2019t have any special ability to remember all these shortened commands; I remember the most-used 10\u201315 commands. And all the commands are printed on paper and kept next to me all the time.\\n\\nThe next question will be, \u201cAm I able to take out and use these short forms when I use other fish terminals in some other place?\u201d Absolutely yes! All the short forms from the fish are stored in a file `abbr.fish` you can take that file or copy its contents to use it anywhere.\\n\\n* **Math/Calculator in Shell**: You can do some amazing calculations with fish shell, all the way from `2+2` to `cos()` and more.\\n\\n![Math in Fish](Math_in_Fish.webp)\\nMath in Fish\\n\\nJust type `math` followed by your math question. And get an instant answer. Using it for a while, I feel opening the calculator and doing some addition/division is a complete waste of time, but fish solve this problem effortlessly. You can learn the power of Fish Math on their dedicated math page here.\\n\\n* **`count` command to count the number of files**: I usually count the number of files in a directory using the bash command `ls | wc -l` to decide which command to use for an operation. For example, if a directory contains 20 files, then we can do GUI-based delete/copy-paste/zip operations. But what if it contains 200,000 files or even 20,000,000 files? In this case, if you use GUI-based operations, it will take multiple days to finish, so you have to switch to the terminal and use efficient commands like `parallel, pigz, dd etc\u2026`. While performing the count operation using ls, cdcommand through different directories, you will waste a lot of time by typing the commands, but `count` in the fish terminal is much easier to use.\\n\\n* **Search through commands using `Ctrl+R`**: I always look into `history` command to get some long/repetitive commands and copy-paste them to save time. For example, `history | grep ssh` to search for the ssh command I use every time, which is far above to reach with the up arrow (navigating the command history). There might be some way to do this in other terminals, but it won\'t be as interactive as Fish.\\n\\n![Using correct command (left)](Using%20correct%20command%20(left);%20Part%20of%20command%20I%20remember%20(right)1.webp)\\n![Part of command I remember (right)](Using%20correct%20command%20(left);%20Part%20of%20command%20I%20remember%20(right)2.webp)\\nUsing correct command (left); Part of command I remember (right)\\n\\nJust press `Ctrl+R` and search for the command you need. In addition to this, Fish supports Fuzzy Searching too, so you just type whatever you remember about the command you\u2019re looking for, Fish will find it for you in a blazing fast way.\\n\\nSo these are the features of Fish I love and use every time. In the coming week I will be dedicating some time to explore other commands like `random, bg, argparse etc\u2026` If you want to explore all the abilities of Fish Terminal yourself, just visit this [command page](https://fishshell.com/docs/current/commands.html).\\n\\nIf you liked this much (fish=continue reading; story=quick clap) then we can set Fish as the default terminal, so that you can use Fish as soon as you launch the terminal.\\n\\n```mdx\\n    which fish \u2014 displays the path/location of fish executable\\n\\n    sudo chsh -s <which fish output here>\\n\\n    or do both together\\n\\n    sudo chsh -s `which fish`\\n```\\n\\n### Problems and solutions\\n\\nWhenever you use a technology, there will be some issues. _When you solve that issue, it will make you closer to that tech, and you will start to love it more and more_. I think it is due to the `kick` we get from solving the issues. So the problem\\n\\n* Conda didn\u2019t work inside Fish\\n\\nWhen I typed `conda` it shown `command not found error`\\n\\n![conda not found in fish](conda%20not%20found%20in%20fish.webp)\\nconda not found in fish\\n\\nI fixed it by\\n\\n```mdx\\n    nano ~/.config/fish/config.fish \u2014 opens fish config file\\n\\n    set -gx PATH /your/path/to/conda/bin $PATH \u2014 Conda Bin path, in my case it was in $HOME/anaconda3/bin\\n\\n    conda init fish \u2014 similar to conda init in bash\\n\\n    source ~/.config/fish/config.fish \u2014 restart the shell or you can log out and log in\\n\\n    conda activate <any conda env name> \u2014 testing\\n```\\n\\n![Locating Conda Bin directory (left)](Locating%20Conda%20Bin%20directory%20(left);%20Setting%20path%20to%20that%20directory%20(right)1.webp)\\n\\n![Setting path to that directory (right)](Locating%20Conda%20Bin%20directory%20(left);%20Setting%20path%20to%20that%20directory%20(right)2.webp)\\nLocating Conda Bin directory (left); Setting path to that directory (right)\\n\\n\\n### Finally\\n\\nSwitching to fish didn\u2019t feel like changing my workflow. It just removed friction. I still run the same commands, ssh into the same servers, and activate the same environments. But now I do it faster, with fewer typos, and less mental overhead. If you use the terminal for most of your work, give Fish a try. You\u2019ll know by then whether it\u2019s for you or not. For me, I\u2019ve already made it my default shell at home and work.\\n\\nNow I am looking forward to more possibilities of Fish, an upgrade to the terminal code editor (currently using nano).\\n\\nSo that\'s it. Thank you for reading!"},{"id":"ai-dangerous-to-youtubers","metadata":{"permalink":"/blog/ai-dangerous-to-youtubers","source":"@site/blog/2025-04-02-How_AI_can_be_Dangerous_to_YouTubers_but_Helpful_to_Viewers/story.md","title":"How AI can be Dangerous to YouTubers but Helpful to Viewers","description":"Read on Medium","date":"2025-04-02T00:00:00.000Z","tags":[{"inline":false,"label":"AI","permalink":"/blog/tags/ai","description":"AI tag description"},{"inline":false,"label":"YouTube","permalink":"/blog/tags/youtube","description":"YouTube tag description"}],"readingTime":11.59,"hasTruncateMarker":true,"authors":[{"name":"Ajay T Shaju","title":"AI & DS Engineer","url":"https://004ajay.github.io/","page":{"permalink":"/blog/authors/ajay"},"socials":{"linkedin":"https://www.linkedin.com/in/ajay-t-shaju/","github":"https://github.com/004ajay","medium":"https://medium.com/@ajaytshaju","x":"https://x.com/004ajayt"},"imageURL":"https://github.com/004ajay.png","key":"ajay"}],"frontMatter":{"slug":"ai-dangerous-to-youtubers","title":"How AI can be Dangerous to YouTubers but Helpful to Viewers","authors":["ajay"],"tags":["ai","youtube"]},"unlisted":false,"prevItem":{"title":"Bash to Fish: A Terminal Change","permalink":"/blog/bash-to-fish"},"nextItem":{"title":"Must Know Windows Key Shortcuts for Your Daily Tasks: A Story like Guide","permalink":"/blog/windows-key-shortcuts"}},"content":"Read on [Medium](https://medium.com/@ajaytshaju/how-ai-can-be-dangerous-to-youtubers-but-helpful-to-viewers-d575310d76f9)\\n\\n\\nIf you\u2019re like me, who puts 200+ videos on different YouTube playlists and does not find time to watch them but your mind does not allow you to remove them, then this blog is for you.\\n\\n\x3c!-- truncate --\x3e\\n\\n![head](head.webp)\\n\\nThis is an experiment I\'ve been doing for a while. It\'s working for me. If you find this helpful or find any problems, please comment.\\n\\nLet\'s come to the point\\n\\nThis blog is about **asking an AI to review a video and give answers to our questions.**\\n\\n![My video chat with Gemini](My%20video%20chat%20with%20Gemini.gif)\\nMy video chat with Gemini\\n\\nI used Google\u2019s AI **Gemini** (previously Bard) to help me get what\u2019s inside a video using the _video link and a question_. I am sharing it here to show the time reduction when I watch the videos on YouTube vs asking Gemini.\\n\\nThe Idea of asking an AI about a video came from the video title itself. For example, a hypothetical title \u201cYou need to stop using this in your Python code\u201d \u2014 when I see this title + video thumbnail, the first question that comes to my mind is \u201cWhat is that one thing I need to stop using in my Python code?\u201d and YouTubers expect people will suddenly click the video to see what it is. This thinking is not wrong, but for a busy person like me \ud83d\ude0c, direct clicking and watching on the spot is not possible, and the video will be added to watch later, then it will stay on the list for many months, still having the curiosity to know what\u2019s that one thing. So, in this kind of situation, a tech lover like me will turn to AI to get the answer. While it justifies my act of using AI in this situation, we lazy humans tend to take shortcuts even if they aren\u2019t required. That\u2019s when AI becomes a bane to some people (here YouTubers).\\n\\nNote: This is not a full-fledged experiment; I am just sharing what I have done and how it saved me time.\\n\\nBefore getting started with anything, I will give you an idea: _\u201cThe Answers from AI depend on your Prompt \u2014 What You Ask, You Get.\u201d_\\n\\n### Why am I doing this?\\n\\nI have more than 200 videos in my YouTube playlist \u2018Watch Later\u2019, 156 videos on another custom playlist named \u2018Hear It\u2019 (videos to be heard), and 55 videos on another custom playlist named \u2018Watch If Time\u2019 (videos where I need to put more effort into watching and doing it side by side). So I planned to go through all the videos in \u2018Watch Later\u2019 to see if I could remove any of those videos and ended up removing only 2 videos. My playlists have more informative videos rather than entertainment videos.\\n\\n![videos in my watch later after removing watched videos](videos%20in%20my%20watch%20later%20after%20removing%20watched%20videos.webp)\\n\\nvideos in my watch later after removing watched videos\\n\\n---\\n\\nNow, let\u2019s dive (ChatGPT vocab) into the details.\\n\\nExperiment Details:\\n\\nNumber of Videos: **18 videos**\\nVideo Languages: **English and Malayalam** (16 English and 2 Malayalam)\\nAI Used: **Google Gemini 2.0 Flash** (As it has good integration with Google services)\\n\\nTime Details:\\n\\nIf I watch all the videos at 1x speed, then it would take about 5 hours and 18 minutes, but if I watch those videos at 1.5x speed then it would take only 3 hours and 30 minutes. Things turned around when I used the above explained AI method and put a stopwatch to record the time taken \u2014 it took me only 1 hour and 24 minutes, where this 1.5 hour included me taking the links of the videos, making questions in my mind, taking notes of Gemini answers, writing down ideas for this blog, etc. If I calculate the time taken for copying the link, making the questions, and getting the answers from AI, it would take only around 45 to 50 minutes. Therefore, I can say I got the answers I was searching for in those videos in just 1 hour rather than 5.3 hours.\\n\\nIn short:\\n\\nTotal length of watching all videos in 1x speed: **5 hours 18 minutes**\\nOn watching in 1.5x speed: **3 hours 30 minutes**\\nWith Gemini AI: **1 hour**\\n\\nGetting started:\\n\\nOpened all videos to different chrome tabs; Went to each video, copied the link; Made the question and asked to Gemini\\n\\n![Videos opened on multiple tabs](Videos%20opened%20on%20multiple%20tabs.webp)\\nVideos opened on multiple tabs\\n\\nNow, with this information in hand, let\'s explore the good and bad sides of this technology, both from users\' and video makers\u2019 perspectives.\\n\\n### How does this help users?\\n\\n- Direct answer for your questions. You will save a lot of time than watching the videos and ADs YouTube throws on you.\\n- Videos may skim through some topics, but on asking an AI, most of the time you will get structured answers and some proper words or phrases that can be used while you are transferring your knowledge in a discussion, interview, or even a casual talk.\\n- Help filter out unwanted information \u2014 take only what you need, for example: the video \u201c7 Best AI Tools You NEED to Try in 2025 (Free & Powerful!) \ud83d\udca1\u201d by [Kevin Stratvert](https://www.youtube.com/@KevinStratvert) has listed tools like Zapier, Google AI studio, ChatGPT, Udio, ElevenLabs, Perplexity, and Descript \u2014 out of these AI tools, Udio was the only one new AI tool for me, so why should I spend 7 minutes and 48 seconds watching his video which costs around 100 to 300 MB of data when I can ask Gemini using the video link and my question and get the answer all within 2 minutes.\\n\\nNow a little Surprise\\n\\nI gave Gemini a link to a **Malayalam video**, which I previously watched; as a surprise to me, it replied with an exact answer. But how has Gemini given answers for a Malayalam video? I tried to make an answer, as I am in the domain of AI and Data Science.\\n\\nThe first answer that came to my mind is that it took some English words the narrator used and collected all the words to give me the answer. But this will give only a surface idea which is not enough, so I though something else, which can be told with a hypothetical situation \u2014 Imagine/remember you listening to two other people talking their language which you don\u2019t know, but they use a language you know (example English) in between, in such scenario you won\u2019t get complete idea, but understands what topic they\u2019re discussing and in what way the talk is going (Positive, Negative, Neutral etc). This is exactly what happened in Gemini.\\n\\nI also asked Gemini to answer how it understood the Malayalam video and provided me with an exact answer. See Gemini\u2019s reply\\n\\n> _I analyze videos using audio processing, visual cues, and contextual understanding. Even without translation, I detect English keywords, filmmaking terms, and on-screen text. Visual analysis helps me grasp settings, characters, and events. My broad film knowledge and pattern recognition allow me to infer meaning from expressions, scene changes, and common storytelling structures. This combination helps me summarize key themes even in languages I don\u2019t fully understand._\\n\\n### What do we miss?\\n\\n* Visual Demonstration: Videos like \u201c30 Must-Know English Idioms for Fluent Conversation!\u201d by POC English \u2014 his acting and graphical illustrations, subtitles with highlights are missed when I am chatting with an AI about the video.\\n\\n* In Coding videos, if you\u2019re a beginner, it is better to see the instructor coding and trying to do it in parallel with the video. But if you\u2019re a professional and just need the core idea to feed your FOMO, then AI is your friend.\\n\\n* The storytelling style, hand and body gestures, and in-depth explanations of Why, What, How, When, etc, are missed, especially the videos from channels like Veritasium.\\n\\n* Audio nuances like the tone of voice, emphasis, pace, etc, which convey important information and transfer emotion like humor, sarcasm, anger, and tension, which are difficult to express in text, are missed.\\n\\n* A video may not be only about the core idea or a term, but it will have useful information in the introduction and conclusion. Gemini like AIs will give only \u2018what you asked for, \u2019 so your questions (prompting) matter.\\n\\n* Also, videos may also give you an experience of discovery \u2014 you may get stuck at any unexpected information, then you will be doing a search to find it, during which you get stuck in some ideas and create something new out of it, or even conceive a new idea.\\n\\n* Potential for Misinterpretation: We need to remember that these AI systems are still an \u2018under-development\u2019 project but a working in most cases, and the LLM providers give a warning about their answers. See the image below.\\n\\n![Warning on using AI](Warning%20on%20using%20AI.webp)\\n\\nWarning on using AI\\n\\n### How can it affect video makers/YouTubers?\\n\\n* Reduced Watch Time: When we use AI, naturally, there will be a decrease in watch time. Watch time is an important metric for YouTube\u2019s algorithm, important for video visibility and monetization. If more viewers use AI summaries instead of watching the videos, creators will end up with watch time troubles.\\n\\n* Engagement Metrics: If a normal viewer likes the video, then they will give a thumbs up, but I don\u2019t think AI users will come back and give thumbs up when using AI. What I do is take time to come back and give a like or comment to respect the creator\u2019s hard work. Also when I am taking the link and asking questions to AI, the video will be playing in the background, so it will give the view too.\\n\\n* Audience Connection: When many audiences go for AI shortcuts, getting a creator-viewer relationship will be much harder. Also, watching videos allows viewers to connect with the creator\u2019s personality, style, and message in a way that summaries cannot replicate. This connection increases loyalty and encourages viewers to return for more content.\\n\\n* Ad Revenue: For all YouTubers, money will be one of the aims and is important, too. If the above explained things happen at a huge rate, then it will affect monetization of the creator.\\n\\n* YouTube\u2019s new update: While I was taking a link of a video, I saw a similar thing to what I was doing, its none other than \u201cAI Summary of the Video directly on YouTube.\u201d\\n\\n![AI video summary on YouTube](AI%20video%20summary%20on%20YouTube.webp)\\nAI video summary on YouTube\\n\\nBy now, regardless of you being a YouTuber or not, you will be thinking \u2014 How can someone become Gemini(AI)-proof?\\n\\nA pause!\\n\\nI was thinking through different methods, and asking AIs to give some ideas \ud83d\ude0e I ended up feeling that it\u2019s impossible to completely \u201cGemini(AI)-proof\u201d, but there is ground for correction \u2014 so you the reader might have some ideas, so store it in your mind or a paper, read my suggestions and if your idea is not listed then come, be a part of making humans AI proof.\\n\\nNow, see some strategies:\\n\\n* Interesting Content: When I see some posts on LinkedIn or Instagram, on reading that I would press the like button multiple times and re-read the post. This kind of curiosity-raising and interesting videos should be created, which makes the viewer watch the video again and again.\\n\\n* Visual Storytelling: Out of the 18 videos taken into the experiment, plus all the other videos I handed over to AI, they have something in common. It\'s none other than FACTS. I look for facts so as similar viewer like me. Videos need to have some facts, but on top of facts, there should be visuals, animations and whatever thing that can make viewer engaged with the video. One should say we have to watch his/her videos. Some examples that come to mind are [Branch Education](https://www.youtube.com/@BranchEducation), [Neo](https://www.youtube.com/@neoexplains), [Fern](https://www.youtube.com/@fern-tv), [3D Living Studio](https://www.youtube.com/@3DLivingStudio), [Jared Owen](https://www.youtube.com/@JaredOwen), etc.\\n\\n* Demonstrations: If your video involves a process, show it clearly. Don\u2019t just describe it with engaging visuals like animations, graphics, and B-roll footage to keep viewers visually engaged.\\n\\n* Dynamic Editing: Keep the pace moving with cuts, zooms, and other editing techniques.\\n\\n* Emphasize Audio Nuances: Use tone of voice, emphasis, and pacing to convey emotion and keep viewers interested. Incorporate music, sound effects, and ambient sounds to create a richer audio experience.\\n\\n* Go Deep on Topics: Provide in-depth explanations, examples, and context that are difficult to summarize. Offer original perspectives and analysis that viewers can\u2019t find elsewhere. Tackle complex subjects that require a longer, more detailed video to fully understand.\\n\\n### What\u2019s next?\\n\\nI have a ton of open tabs in my Google Chrome, both on mobile and laptop. Most of them are to be read, related to developments and topics in AI & DS and some geo-politics.\\n\\nRecently, I noticed a weird thing appearing on the tab count placeholder in Google Chrome Mobile App, it was **:D**. At first, I thought it was a bug in Chrome, but when I read and closed some tabs, I saw number 97 as the tab count. I tried adding three more tabs and saw the symbol again, and when I searched for the meaning of :D, it was a big smile emoji \ud83d\ude03.\\n\\n![99 tabs and 100 tabs in Google Chrome1](99%20tabs%20and%20100%20tabs%20in%20Google%20Chrome1.webp)\\n![99 tabs and 100 tabs in Google Chrome2](99%20tabs%20and%20100%20tabs%20in%20Google%20Chrome2.webp)\\n99 tabs and 100 tabs in Google Chrome\\n\\nNow, it\u2019s time to tackle the problem of too many tabs in Chrome. Until I find a quick solution, a short bye \ud83d\udc4b\\n\\nBefore leaving, please clap and share this story if it adds value."},{"id":"windows-key-shortcuts","metadata":{"permalink":"/blog/windows-key-shortcuts","source":"@site/blog/2024-11-25-Must_Know_Windows_Key_Shortcuts_for_Your_Daily_Tasks_A_Story_like_Guide/story.md","title":"Must Know Windows Key Shortcuts for Your Daily Tasks: A Story like Guide","description":"Read on Medium","date":"2024-11-25T00:00:00.000Z","tags":[{"inline":false,"label":"Windows","permalink":"/blog/tags/windows","description":"Windows tag description"},{"inline":false,"label":"Shortcuts","permalink":"/blog/tags/shortcuts","description":"Shortcuts tag description"}],"readingTime":6.67,"hasTruncateMarker":true,"authors":[{"name":"Ajay T Shaju","title":"AI & DS Engineer","url":"https://004ajay.github.io/","page":{"permalink":"/blog/authors/ajay"},"socials":{"linkedin":"https://www.linkedin.com/in/ajay-t-shaju/","github":"https://github.com/004ajay","medium":"https://medium.com/@ajaytshaju","x":"https://x.com/004ajayt"},"imageURL":"https://github.com/004ajay.png","key":"ajay"}],"frontMatter":{"slug":"windows-key-shortcuts","title":"Must Know Windows Key Shortcuts for Your Daily Tasks: A Story like Guide","authors":["ajay"],"tags":["windows","shortcuts"]},"unlisted":false,"prevItem":{"title":"How AI can be Dangerous to YouTubers but Helpful to Viewers","permalink":"/blog/ai-dangerous-to-youtubers"},"nextItem":{"title":"Converting Bank Statements to Insights: Automating Future Data Collection using Gmail Zapier and Google Sheets Part 2","permalink":"/blog/statement-to-insights-part2"}},"content":"Read on [Medium](https://medium.com/@ajaytshaju/must-know-windows-key-shortcuts-for-your-daily-tasks-a-story-like-guide-97153461faed)\\n\\nAbout 73% of world computer users use Windows as their computer\u2019s operating system([see source](https://gs.statcounter.com/os-market-share/desktop/worldwide/#daily-20240921-20241020)). In this group, 35% of users use Windows 11 ([see source](https://gs.statcounter.com/windows-version-market-share/desktop/worldwide/#monthly-202106-202410)). In this story-like guide, I will explain some useful Windows key shortcuts we can use daily!\\n\\n\x3c!-- truncate --\x3e\\n\\n_For your information: I will use **Win** as a short-term for Windows. I am using Win 11 but I believe most of the shortcuts in this story will work on other Windows versions as well._\\n\\n![Windows Key (Image by Author)](Windows%20Key.webp)\\nWindows Key (Image by Author)\\n\\n### A Day in My Life: Starting by booting Windows\\n\\nMost days, I feel the initial energy as I boot up my computer. The days may be full of simple tasks but if left normal, they will take up a good amount of time \u2014 browsing, checking emails, opening documents, and switching between apps. We all do these tasks, but to be honest: using a mouse for everything is similar to walking in a race (a satiric approach \ud83d\ude04). I\u2019m a keyboard shortcut enthusiast, see my [Jupyter Notebooks story](https://medium.com/@ajaytshaju/useful-keyboard-shortcuts-in-jupyter-notebook-3d488c1e5d29) and you will understand. So let\'s start with music.\\n\\nThe first thing I do after booting my laptop is to press **Win + A** to open the **Action Center** and toggle Bluetooth ON to pair my wireless earbuds. To start playing some music, I press **Win + 2** ( Chrome browser, the **second item on my taskbar**), straight to YouTube, play my Malayalam music playlist to give me a push for minimal tasks.\\n\\n![Action Center \u2014 Win+A](Action%20Center.webp)\\n\\nAction Center \u2014 Win+A\\n\\nAccess apps pinned on the taskbar using _Win + corresponding number_, in the image below, to **access the YouTube** I press **Win + 4**, to access the **WhatsApp Web** I press **Win + 5**, and so on. _Count the app number from 1 after the \u2018Start/Windows\u2019 Icon._\\n\\n![Taskbar Apps](Taskbar%20Apps.webp)\\n\\nTaskbar Apps\\n\\n### Getting started with some tasks\\n\\nToday\u2019s first task is to collect, arrange, and move some PDFs to different folders, to get started with this task I need to open the Win File Explorer. We have multiple ways to open the File Explorer:\\n\\n* Option 1: Press **Win + D** or **Win + M** to **minimize all windows** and then press **Win + 1** to open Explorer from the taskbar.\\n* Option 2: Use the faster route \u2014 **Win + E** to **open Explorer directly.**\\n* Option 3: **Cycle through taskbar apps** using **Win + T**.\\n\\nWith Explorer open, I can make folders using **Ctrl + Shift + N**, locate my PDFs, make a few edits, and rearrange them. Thanks to my trusty shortcuts the whole process is seamless and quick.\\n\\n_Short Tip: **Win + D** and **Win + M** do the same task of minimizing windows. But when you press **Win + D** it will minimize all the windows, and it restores the same windows to their previous state when you press **Win + D** again. This is different from **Win + M**, which minimizes all windows but doesn\u2019t bring them back with a second press._\\n\\nWhile I was editing and rearranging the 13th PDF, my mother asked me to write a letter to someone. She explained the content in our local language Malayalam. To save time, I opened Windows Notepad and pressed **Win + H** to open the **dictation feature** for quick writing without typing. As you may have guessed, I wrote 60% of my medium blogs using this feature. But proofreading was essential to catch any miswritten words or missing details. See the image below.\\n\\n![Ouput after using Win + H dictation feature](Ouput%20after%20using%20Win%20+%20H%20dictation%20feature.webp)\\nOuput after using Win + H dictation feature\\n\\nThough it seems overwhelming to speak, read, and correct mistakes, it is often time-saving and good for fingers than typing. After I completed the letter, I sent the content via WhatsApp.\\n\\nBefore continuing with other works I saw a charge draining problem with my laptop\'s battery, to inspect it I opened **Windows Settings** using **Win + I** to see the battery performance on System > Power & battery but I didn\u2019t see any problem in the stats shown by my computer. For more scrutiny, I pressed **Win + X** to open the **quick link menu** (same as right-clicking on the start button) opened the terminal in Admin mode, and used the command `powercfg /batteryreport` to generate a detailed report of my battery \u2014 which shows my battery capacity has dropped from 66Wh to 40Wh in just three years of usage.\\n\\n![Battery Report using terminal command](Battery%20Report%20using%20terminal%20command.webp)\\nBattery Report using terminal command\\n\\nSeeing the poor condition of my laptop\'s battery I felt a need for a short break for my laptop as well as to recharge myself. I pressed **Win + L** to **lock my laptop** and stepped away for a quick stretch and refresh.\\n\\n### The Next Cycle\\n\\nI had more things to do, returned to my laptop. The first task is to help one of my friends work with projectors on Windows. I set up a Google Meet and demonstrated it live using **Win + P** to access **Projection Settings**. This shortcut makes switching between modes like Duplicate and Extend easier, I also taught him screen size and resolution adjustments.\\n\\nNext, I was planning for a small tweak \u2014 adding the _Open with VS Code_ option to my right-click menu for folders. I press **Win + R** to open the Run dialog box, typed `regedit`, and hit Enter. In the registry editor, I did some customization in no time. And the result was amazing, just click the underlined option in the context menu to open vs code on any folder.\\n\\n![Open with VS Code on Context Menu](Open%20with%20VS%20Code%20on%20Context%20Menu.webp)\\n\\nOpen with VS Code on Context Menu\\n\\nWhile I was doing a lazy walk through some websites, I found a webinar from an MNC and filled out a webinar registration form. Just like other forms, it consists of many redundant things like name, email, address, pin code, college, etc. I used **Win + V** to access **Clipboard History** and seamlessly pasted all the information as I had copied and pinned it on my clipboard way back.\\n\\n![A snapshot of my clipboard](A%20snapshot%20of%20my%20clipboard.webp)\\n\\nA snapshot of my clipboard\\n\\nNow it\'s time to give you, the reader something. I pressed **Win + ,** (comma) to temporarily **peek at my desktop** to show you my latest minimal name typography wallpaper.\\n\\n![My latest wallpaper](My%20latest%20wallpaper.webp)\\nMy latest wallpaper\\n\\nAnd last but not least, in chats and blogs, emojis convey emotions well. Add emojis while working on Windows laptops using **Win + .** (period), to open the **emoji window** and click to add emoji wherever needed \ud83d\ude05\ud83d\ude05.\\n\\n_Bonus tip: In the WhatsApp desktop or web app, type `:` (colon) followed by the emoji name (e.g., `:happy`) for faster emoji insertion._\\n\\nSince most of my works were finished, it\u2019s time to give my laptop a good night\'s sleep. I pressed a series of shortcuts \u2014 **Win + D, Win + X, U, S**, where **Win + D** minimizes all windows, **Win + X** opens the quick link menu, **U** for Shut down or sign out, and **S** for Sleep. This is my favorite shortcut\ud83d\udc93.\\n\\n![Setting laptop to sleep using Win + X, U, S](Setting%20laptop%20to%20sleep.webp)\\n\\nSetting laptop to sleep using Win + X, U, S\\n\\n### Sum up all\\n\\nHere is a list:\\n\\n    \\n**Win + A**: Open Action Center\\n\\n**Win + D**: Show or hide desktop\\n\\n**Win + E**: Open Explorer\\n\\n**Win + H**: Dictation feature\\n\\n**Win + I**: Open settings\\n\\n**Win + L**: Lock PC/Switch account\\n\\n**Win + M**: Minimize windows\\n\\n**Win + P**: Projection settings\\n\\n**Win + R**: Open Run dialog box\\n\\n**Win + T**: Cycle through apps on the taskbar\\n\\n**Win + V**: Clipboard history\\n\\n**Win + X**: Quick link menu (same as right-click on the Start button)\\n\\n**Win + , (comma)**: Temporary peek at desktop\\n\\n**Win + . (period)**: Open emoji panel"},{"id":"statement-to-insights-part2","metadata":{"permalink":"/blog/statement-to-insights-part2","source":"@site/blog/2024-11-13-Part_2_Converting_Bank_Statements_to_Insights_Automating_Future_Data_Collection_using_Gmail_Zapier_and_Google_Sheets/story.md","title":"Converting Bank Statements to Insights: Automating Future Data Collection using Gmail Zapier and Google Sheets Part 2","description":"Read on Medium","date":"2024-11-13T00:00:00.000Z","tags":[{"inline":false,"label":"Python","permalink":"/blog/tags/python","description":"Python tag description"},{"inline":false,"label":"Automation","permalink":"/blog/tags/automation","description":"Automation tag description"}],"readingTime":12.99,"hasTruncateMarker":true,"authors":[{"name":"Ajay T Shaju","title":"AI & DS Engineer","url":"https://004ajay.github.io/","page":{"permalink":"/blog/authors/ajay"},"socials":{"linkedin":"https://www.linkedin.com/in/ajay-t-shaju/","github":"https://github.com/004ajay","medium":"https://medium.com/@ajaytshaju","x":"https://x.com/004ajayt"},"imageURL":"https://github.com/004ajay.png","key":"ajay"}],"frontMatter":{"slug":"statement-to-insights-part2","title":"Converting Bank Statements to Insights: Automating Future Data Collection using Gmail Zapier and Google Sheets Part 2","authors":["ajay"],"tags":["python","automation"]},"unlisted":false,"prevItem":{"title":"Must Know Windows Key Shortcuts for Your Daily Tasks: A Story like Guide","permalink":"/blog/windows-key-shortcuts"},"nextItem":{"title":"Converting Bank Statements to Insights: Automating Future Data Collection using Gmail Zapier and Google Sheets Part 1","permalink":"/blog/statement-to-insights-part1"}},"content":"Read on [Medium](https://medium.com/@ajaytshaju/converting-bank-statements-to-insights-automating-continuous-data-collection-and-building-a-c7e474f329b1)\\n\\nBefore starting Part 2, make sure you catch up on Part 1, where I share the groundwork for transforming raw bank statements \u2014 the data collection, Google Sheets and Llama LLM for data cleanup, failed attempts, and learning many valuable techniques \u2014 click to read Part 1!\\n\\n\x3c!-- truncate --\x3e\\n\\n![The overall process](The%20overall%20process.webp)\\nThe overall process\\n\\n## What\u2019s in this part\\n\\n    The Search for a fast way to Fill 940+ Dropdowns\\n    Making the Dashboard\\n    Automating Future Data Collection: The powerful integration between Gmail, Zapier, and Google Sheets\\n    What to do in the Future? Advantages & Disadvantages\\n\\n### 1. The Search for a fast way to Fill 940+ Dropdowns\\n\\nThe first way is to fill each category by clicking and selecting one by one from a list of 70+ categories, but it would take a lot of time, maybe an average of 15 seconds(approx.) for each dropdown, considering, the entries in the Details column may not be correct; they may have dual meanings; the category may change with the type of transaction(credit/debit), then it would take 14,130 seconds, which is about 4 hours. This scrutiny of checking my transaction history from the UPI app or Bank Statements to choose the category is important if I need to do Machine Learning or Deep Learning tasks later, as they need quality data for better outputs.\\n\\n![Dropdown selection-one by one](Dropdown%20selection-one%20by%20one.webp)\\nDropdown selection-one by one\\n\\nSince speed is important I had to look for another way to speed up the dropdown selection process, I thought why not use some spreadsheet formula to do this task \u2014 I found some formulas. Use IF() to find the key term(\u2018Petrol Pump\u2019, \u2018Ajay Bakers\u2019 \ud83d\ude02) in the details column and put a suitable category(\u2018Scooter Petrol\u2019, \u2018Bakery\u2019) and the formula looks like\\n\\n`=IF(ISNUMBER(SEARCH(\\"Petrol Pump\\", D2)), \\"Scooter Petrol\\", IF(ISNUMBER(SEARCH(\\"Grocery Store\\", D2)), \\"Household\\", \\"\\"))`\\n\\nSee how this formula works\\n\\n* =IF(ISNUMBER(SEARCH(\u201cPetrol Pump\u201d, D2)), \u201cScooter Petrol\u201d, \u201c\u201d))\\n\\n* =IF(ISNUMBER(1), \u201cScooter Petrol\u201d, \u201c\u201d))\\n\\n* =IF(TRUE, \u201cScooter Petrol\u201d, \u201c\u201d))\\n\\n* Value \u201cScooter Petrol\u201d in the cell where this formula exists\\n\\n_The ISNUMBER() returns TRUE if SEARCH() finds the specified keyword(here, \u2018Petrol Pump\u2019 or \u2018Grocery Store\u2019) allowing the IF() function to display the term if the condition is True. Else SEARCH() returns Error, ISNUMBER() will return FALSE, and IF() could handle the False case as specified._\\n\\nThis works but the problem with this approach is\\n\\n1. Currently I have 70+ categories so the formula will become lengthy and complex with many nested IF cases.\\n2. The output of this formula is a text, not a chosen item from a dropdown, so if I need to change the entry in the future it would be not easy.\\n\\n_The solution was to use Data Validation with VLOOKUP() in a Reference Table_\\n\\nThe reference table has two columns \u2018Keyword\u2019 and \u2018Category\u2019 having \u2018Petrol Pump\u2019 \u2014 \u2018scooter petrol\u2019, \u2018Grocery Store\u2019 \u2014 \u2019household\u2019 respectively and the formula looks like\\n\\n`=IFERROR(VLOOKUP(A2, ReferenceSheet!A:B, 2, FALSE), \\"\\")`\\n\\nBut this approach has some problems, they are\\n\\n1. Every time the keyword won\u2019t be \u2018petrol pump\u2019 or \u2018XX Store\u2019 but different words like \u2018Diesel Pump\u2019, \u2018XY Store\u2019, etc \ud83d\ude0c, and the details entry won\u2019t have a consistent letter-case, sometimes entry will be full capital letters or small letters, in other cases its a mix of small and capital letters.\\n2. (same as above)\\n\\nSolving this is another nightmare with more complex tables and formulas like `=IFERROR(INDEX(Categories, MATCH(TRUE, ISNUMBER(SEARCH(LOWER(ReferenceSheet!D:D), LOWER(D2))),0)), \\"\\")`\\n\\nAs these formulas got overly complex I thought of going back to my previous idea of selecting each category one by one, after doing this for a while, due to the repetition, I switched to a lazy approach \u2014 Filling the categories \u2018randomly\u2019 rather than one by one.\\n\\n![Dropdown selection-random](Dropdown%20selection-random.webp)\\nDropdown selection-random\\n\\nAnd laziness version 1.2 was copying a category and pasting it to similar items\ud83d\ude0c. With this approach, I can save somewhat 1 hour, but I got a spark quickly \u26a1- that is to give a small change that could increase the speed twice and the idea is to use \u2018Find\u2019 to locate some key terms(for ex, \u2018KL Pump\u2019) in the details column and pasting the category copied before(\u2018Scooter Petrol\u2019). This dropped the 15-second time to 6 seconds, if we do the math it is only 5,652 seconds ~ 1.5 hours as compared to 4 hours for one-by-one manual selection.\\n\\n![Dropdown selection \u2014 copy-find-paste](Dropdown%20selection%20\u2014%20copy-find-paste.webp)\\nDropdown selection \u2014 copy-find-paste\\n\\nAnother problem is looking at the ocean-like data, cross-matching the details, and selecting the categories that lead to oozing out of energy, this would happen for most people. I tackled this problem by setting an egg-shaped Pomodoro timer right next to me, with each cycle spanning 25 minutes with a 5-minute break, and repeating the cycle 4\u20136 times to reach 2\u20133 hours of productive work. Besides this, I set up a visual cue(counter) to let me know the task is moving forward by showing me how many dropdowns are remaining, using the formula `=942-COUNTA(E2:E944)`,the output looks like Dropdown to be completed: 471.\\n\\n### 2. Making the Dashboard\\n\\n![View of My Minimal Dashboard](View%20of%20My%20Minimal%20Dashboard.webp)\\nView of My Minimal Dashboard\\n\\nBefore starting to build the dashboard I gave more details to my data such as adding the Main Category and Type(Income/Expense) using the XLOOKUP formula to match with the category name. The XLOOKUP formula used for putting Main Category using sub-category was `=XLOOKUP(E2, $O$2:$O$100,$P$2:$P$100)`\\n\\n![XLOOKUP Formula](XLOOKUP%20Formula.webp)\\nXLOOKUP Formula\\n\\nBuilding dashboards with three or four charts takes many steps when explained as text, if you like to learn more take a look at one of my other stories ([Step-by-Step Guide for Creating a Reading Habit Tracker in Google Sheets](https://medium.com/@ajaytshaju/step-by-step-guide-for-creating-a-reading-habit-tracker-in-google-sheets-96920411282f)) to see how I created charts with steps explained. Here in this project, I kept the Dashboard minimal similar to my other trackers. In this minimal dashboard what I like the most is the Pivot Tables, they give a bird eye view of my finances. I added \u2018Category\u2019 in Row, \u2018Year\u2019 in Column, \u2018Month\u2019 in Filters, and \u2018Debit\u2019 and \u2018Credit\u2019 in Values in two pivot tables(one for debit and the other for credit), which enabled me to see the transaction totals Year-wise and filter by Month.\\n\\n### 3. Automating Future Data Collection: The Powerful Integration Between Gmail, Zapier, and Google Sheets\\n\\nWhenever I send/receive some money, I get both a message and an email notification, which feels annoying. Surprisingly, this \u201cdistress\u201d ended up being useful for automating future data collection!\\n\\nHere\u2019s how I approached it: I connected three applications \u2014 Gmail, Zapier, and Google Sheets \u2014 using Zapier as the link between Gmail and Google Sheets. Essentially, Zapier pulls emails matching a specific search term (like credit or debit notifications from Axis Bank), formats them using its built-in AI, and then adds a row to Google Sheets.\\n\\nNow let\u2019s understand the Zapier processes:\\n\\n![Zapier Interface](Zapier%20Interface.webp)\\nZapier Interface\\n\\nZapier has an amazing landing page, we can text Zapier AI what is out automation plan and within a few seconds it will give the partially initialized step-by-step automation process and we just need to click the \u2018try it\u2019 button to bring the AI-created automation steps to our automation editor. By the way, Zapier calls their automation \u2018Zap\u2019.\\n\\n![Power of Zapier AI](Power%20of%20Zapier%20AI.webp)\\nPower of Zapier AI\\n\\nWhen our Zap is in the editor we will have a Copiolt AI at our disposal with which we can ask our doubts, configure the steps, and many more!\\n\\n![Zap Editor](Zap%20Editor.webp)\\nZap Editor\\n\\nOverall, setting up my automation took me only 10 minutes to connect the applications and test the functionalities. After your first Zap, the time to make the next will come down by 2\u20134min; it\'s that easy to use Zapier.\\n\\nWhile your Zap is at work, you can see the logs on \u2018Zap Runs\u2019 by clicking on the icon placed left side of your Zap editor. When your Zap fails Zapier will notify you through email of the cause and what to do next. See one such email Zapier sent me when my Zap failed.\\n\\n![Zapier alert email](Zapier%20alert%20email.webp)\\nZapier alert email\\n\\nWhen I checked the problem on my Zap editor I got to know the issue in detail \u2014 It was an error with the Google Sheets, it may be down for update, maintenance, or lack of resources.\\n\\n![Zap failed to add sheet row](Zap%20failed%20to%20add%20sheet%20row.webp)\\nZap failed to add sheet row\\n\\nSo far Zapier has automatically added 28 transactions to my Google Sheet, but it will bring only the Date, Details, and Credit/Debit amount, all the other columns should be filled by me \u2014 the only relief is that I need to extend the formula for all remaining columns after filling the Category column (I am taking this manual step as a review for my transactions).\\n\\n![Data collected automatically using Zapier](Data%20collected%20automatically%20using%20Zapier.webp)\\nData collected automatically using Zapier\\n\\nSince Zapier reads and writes data from my accounts, naturally, privacy is a concern. Based on my research, Zapier uses bank-level AES-256 encryption and complies with various data protection frameworks. Additionally, it hosts data on AWS servers in the U.S., including personal data and data processed on behalf of customers. You can find more details about Zapier\u2019s encryption and security policies here: https://zapier.com/legal/data-privacy.\\n\\n## A problem with Zapier\\n\\nSince I don\u2019t want to spend any money on this project, one issue I\u2019m currently facing is the expiration of the professional trial of Zapier, which only lasts for 14 days. After this trial, my account will revert to the free version, and as per their free account policies, it does not allow multi-step Zaps. Unfortunately, my automation requires three steps: retrieving data from Gmail, transforming the data using Zapier AI, and loading it into a new row in the spreadsheet. These steps are essential for the process to work correctly. If you have any ideas to replace the automation step or any other trustworthy platforms that offer this kind of automation, please feel free to share in the comments.\\n\\n### 4. What to do in the Future?\\n\\n1. I have to check the data in fixed intervals to correct any errors that may happen to Zapier while copying the data from Gmail, as the mail is not coming in a suitable format to be uploaded to a spreadsheet directly. In addition to filling the columns which were not filled by Zapier.\\n\\n2. I have to analyze the data by selecting some values on slicers and filters to get insights or even downloading and uploading the data to a SQL Database/Python to query it on more complex constraints to answer questions like \u2018What was my total expense on clothing and personal care for the last two months of recent years? Is there any trend of buying certain items?\u2019 etc.\\n\\nInsights I get from these visualizations\\n\\n* **Insight:** Total money spent on monthly mobile recharge is equal to doing yearly plan. **New decision:** Can think of yearly mobile recharge. **Priorities:** Switching to another service(ex, BSNL)\\n\\n* **Insight:** Total money spent on groceries in a month shows that bulk buying from a wholesale store is cheaper. **New Decision:** Can consider buying groceries in bulk. **Priorities:** Need more storage space and potential chances of food spoilage.\\n\\n* **Insight:** The electricity bill trend is going up over time. New **Decision:** Upgrading to energy-efficient appliances or installing Solar Panels. **Priorities:** Balancing with needs and wants, impact on overall finances.\\n\\nNow let\'s go through the advantages & disadvantages of this project\\n\\n## Advantages\\n\\n* Reduced Manual Data Entry: Automation minimizes manual input, saving time and reducing data entry errors.\\n\\n* Real-Time Data Updates: The dashboard updates instantly with every transaction email, keeping my financial records up-to-date.\\n\\n* Scalable and Adaptable: The system efficiently handles increasing transactions with minimal manual effort. Compared to the insights and control it provides, the manual tasks I have to perform are minor.\\n\\n* Centralized Financial Tracking: Google Sheets offers a single view of my transaction history, balances, and spending habits. I can access it conveniently from any device with internet access, allowing me to manage my finances on the go.\\n\\n* Customizable Dashboard: I can modify the dashboard layout and charts to suit my personal tracking preferences.\\n\\n* Cost-Effective: Using free or low-cost tools like Gmail, Zapier, and Google Sheets makes the system affordable. I can also link multiple bank accounts without significant added cost. For higher data volume and multiple accounts, \u2018Google Apps Script\u2019 can improve writing speed and formatting.\\n\\n* Enhanced Financial Awareness: Regular tracking helps me understand spending patterns and improve budgeting over time. Simple pivot charts of past transactions have supported decisions, like opting for yearly mobile recharges, reallocating funds to different categories, and controlling spending in some areas.\\n\\n## Disadvantages\\n\\n* Manual Updates/Not fully automated: When Zapier inputs data, it doesn\u2019t categorize transactions automatically, split dates into months and years, or calculate the balance. I need to update these manually using the TEXT formula and extend the other formulas to new rows.\\n\\n* Dependency on Email Notifications: The system relies on timely transaction emails, which can cause errors if emails are delayed or missed.\\n\\n* Zapier Costs: Currently, my transaction count fits within Zapier\u2019s free version, but if it grows, I may need to either reduce transactions or switch to a paid plan.\\n\\n* Limited Scope: This system is designed for simple transactions, but if we need to expand it to track other financial data like investments, credit scores, and net worth it would add complexity.\\n\\n## Failure points of my system\\n\\n* It depends on my bank\u2019s email notification, if my bank\u2019s mailing service is down then the transaction won\u2019t be added to my tracker, as missed emails won\u2019t be sent again when the system comes online.\\n\\n* Failure/downtime in the mediator service(Zapier in my case) would cause some transactions to be not added.\\n\\n* _In short: if I add more apps or services, their downtimes and failures will affect this tracker._\\n\\n* If I make a hand-to-hand money transfer, for example: I give Rs.60 to my nephew for buying 2 packets of milk; there is no Gmail or bank involved directly, so I need to account for it manually.\\n\\nNow you may have some suggestions, kindly post them in the comments. There is a lot to be corrected in this, for example \u2014 if you know how to work with HTML and CSS, then delete the div container codes and render a webpage full of credit and debit info without any dividing boxes and just copy the data directly from the webpage and apply some regular expression to remove unwanted information(from [Part 1](http://www.medium.com/@ajaytshaju/converting-bank-statements-to-insights-using-google-sheets-for-data-transformation-and-cleanup-58259d102108)) would be much better than chewing the raw HTML CSS codes. I may return with another blog in the future by solving the current processes(maybe the app that solves my current issues might be under the work of a thinker like you). With that said, Thank you for reading \ud83c\udf89."},{"id":"statement-to-insights-part1","metadata":{"permalink":"/blog/statement-to-insights-part1","source":"@site/blog/2024-11-09-Part_1_Converting_Bank_Statements_to_Insights_Using_MS_Excel_and_Google_Sheets_for_Data_Transformation_and_Cleanup/story.md","title":"Converting Bank Statements to Insights: Automating Future Data Collection using Gmail Zapier and Google Sheets Part 1","description":"Read on Medium","date":"2024-11-09T00:00:00.000Z","tags":[{"inline":false,"label":"Python","permalink":"/blog/tags/python","description":"Python tag description"},{"inline":false,"label":"Automation","permalink":"/blog/tags/automation","description":"Automation tag description"}],"readingTime":12.05,"hasTruncateMarker":true,"authors":[{"name":"Ajay T Shaju","title":"AI & DS Engineer","url":"https://004ajay.github.io/","page":{"permalink":"/blog/authors/ajay"},"socials":{"linkedin":"https://www.linkedin.com/in/ajay-t-shaju/","github":"https://github.com/004ajay","medium":"https://medium.com/@ajaytshaju","x":"https://x.com/004ajayt"},"imageURL":"https://github.com/004ajay.png","key":"ajay"}],"frontMatter":{"slug":"statement-to-insights-part1","title":"Converting Bank Statements to Insights: Automating Future Data Collection using Gmail Zapier and Google Sheets Part 1","authors":["ajay"],"tags":["python","automation"]},"unlisted":false,"prevItem":{"title":"Converting Bank Statements to Insights: Automating Future Data Collection using Gmail Zapier and Google Sheets Part 2","permalink":"/blog/statement-to-insights-part2"},"nextItem":{"title":"Step by Step Guide for Creating a Reading Habit Tracker in Google Sheets","permalink":"/blog/reading-habit-tracker-google-sheets"}},"content":"Read on [Medium](https://medium.com/@ajaytshaju/converting-bank-statements-to-insights-using-google-sheets-for-data-transformation-and-cleanup-58259d102108)\\n\\nWe all spend 12\u201315+ years studying, aiming to secure a job or start a business \u2014 in other words, to make money. Most of us succeed in one way or another. _As Malayalam actor Prithviraj Sukumaran once said in an interview, \u201cGetting to a position is easier, but maintaining it is harder\u201d._ The same applies to money\u2014earning, whether from a job, investments, real estate, rentals, or even through family support may come relatively easily, but managing it wisely is much more challenging. That\u2019s where financial trackers become essential. They allow us to visually track where our money comes from and where it goes, enabling us to make informed decisions. This blog will guide you in building a simple yet automated personal finance tracker.\\n\\n\x3c!-- truncate --\x3e\\n\\n_For your information:_ After finishing the blog, I realized it has a reading time of about 35 to 40 minutes, which is too long for any reader. So, I decided to split the story into two parts.\\n\\n>_Part 1 (this story): Converting Bank Statements to Insights: Using Google Sheets for Data Transformation and Cleanup._\\n\\n>_Part 2: Converting Bank Statements to Insights: Automating Continuous Data Collection and Building a Self-Updating Dashboard with Gmail, Zapier, and Google Sheets._\\n\\n## Part 1\\n\\n* Backstory\\n\\n* Data Collection\\n\\n* Data Transformation\\n    \\n    * Converting HTML File to CSV Format \u2014 The Failed Attempt.\\n    \\n    * What Led to the Failure of Old Transaction Data Collection?\\n\\n* Arranging and Cleaning Axis Bank Transaction Data\\n\\n---\\n\\n### 1. The Backstory\\n\\nI started seriously thinking about managing and tracking my finances in late September 2024. By the end of September, I explored how others handle personal finance tracking by checking out sample templates on Google Sheets and Microsoft Excel and watching YouTube videos. Then, I started the data collection by the start of October 2024, however, due to the heavy work involved in logging the transactions, either by writing it down and then typing it into a spreadsheet or typing the data directly into the spreadsheet, I didn\u2019t do the data collection task correctly. This time I realized why seasoned data professionals say \u2018collecting data is the most tiring process\u2019. So I found an easier way to do this process using \u2018Google Forms to populate the Google Sheet\u2019 \u2014 I made a Google form with questions Date, Details, and Credit/Debit and kept the link to this form in an easily accessible place, but this also has a downside, as this is similar to typing directly into google sheets but instead of opening the sheet app I open the forms app. Therefore that step also failed, in short \u2014 I failed fast and searched for other options.\\nAfter a few days, I noticed a pattern: I was using Google Pay for most of my transactions. Each time I used Google Pay or withdrew money from an ATM, a notification was sent to me as a message to my phone and mail to my Gmail. While I initially saw these emails as redundant, I soon realized they could be useful for automating my data entry. I thought, why not extract transaction data from these emails and enter it directly into my Google Sheet?\\nTo automate this, I needed a service that could access my Gmail, identify whether a mail is about a bank transaction, then identify whether it is about credit or debit, and finally format and upload the data to my Google Sheet. As the mediator, I chose Zapier a reliable tool with bank-level encryption, so I could trust it while working with sensitive data.\\nWith Zapier, Gmail, and Google Sheets, my personal finance tracking system started to take shape, as shown in the diagram below.\\n\\n![My Automated Personal Finance Tracking System](My%20Automated%20Personal%20Finance%20Tracking%20System.webp)\\nMy Automated Personal Finance Tracking System\\n\\n---\\n\\n### 2. Data Collection\\n\\nI used two bank accounts \u2014 Canara Bank and Axis Bank. My current account is with Axis Bank, while my Canara Bank account was deactivated two years ago. To start, I needed to gather data from both banks.\\nI accessed Axis Bank\u2019s Internet Banking services.\\n\\n![Axis Bank Internet Banking Interface](Axis%20Bank%20Internet%20Banking%20Interface.webp)\\nAxis Bank Internet Banking Interface\\n\\nNavigated to Account Statements, and downloaded my statements as CSV files. Since Axis Bank only allows transaction exports for up to 366 days at a time, I had to download two files. The first covered February 2023 to February 2024, and the second included February-March 2024 to October 2024. Below, you\u2019ll see images of these two CSV files.\\n\\n![Two CSV files containing my Axis Bank transactions](Two%20CSV%20files%20containing%20my%20Axis%20Bank%20transactions.webp)\\n\\nTwo CSV files containing my Axis Bank transactions\\n\\nHowever, I ran into an issue when trying to retrieve data from Canara Bank. After deactivating my Canara Bank account, I deleted all related emails, account statements, and notifications from Gmail, severing my connection with the bank. This data would have been valuable, especially for machine learning or deep learning projects where more data enhances model accuracy. With records dating back to 2021, I could have added around 1,500 rows of data. In an attempt to recover this information, I searched Google Pay for any 2021 transaction data related to Canara Bank and found some. I used Google Takeout to get my Google Pay data but discovered that the CSV file where the data can be found is empty, I tried requesting the data multiple times while writing this blog and sent Google feedback about this problem, but still no data is seen in the money send and requests CSV file from Google Takeout.\\n\\n![Empty money send and requests CSV file from Google Takeout](Empty%20money%20send%20and%20requests%20CSV%20file%20from%20Google%20Takeout.webp)\\nEmpty money send and requests CSV file from Google Takeout\\n\\nHowever, I found an HTML file showing a simple web page with my transactions, including the amount, date, time, and some basic details.\\n\\n![Folder tree of Google Pay data takeout](Folder%20tree%20of%20Google%20Pay%20data%20takeout.webp)\\nFolder tree of Google Pay data takeout\\n\\nUnfortunately, this HTML file posed another challenge since it wasn\u2019t straightforward to convert this into CSV format. In the next section, I will explain the series of tasks I followed to extract the data.\\n\\n---\\n\\n### 3. Data Transformation\\n\\nConverting HTML File to CSV Format \u2014 The Failed Attempt.\\n\\nI started by opening the MyActivity.html file in VS Code, only to see an overwhelming amount of unreadable characters. To make sense of it I selected all the text, right-clicked, and pressed the Format Document option (Keyboard shortcut: Shift + Alt + F). VS Code then automatically structured the text, making it much cleaner and easier to read.\\n\\n![Before and After view of formatting the code on VS Code](Before%20and%20After%20view%20of%20formatting%20the%20code%20on%20VS%20Code.webp)\\nBefore and After view of formatting the code on VS Code\\n\\n![Line number and Div container structure](Line%20number%20and%20Div%20container%20structure.webp)\\nLine number and Div container structure\\n\\nI examined the readable file and discovered around 8,000 lines of code were CSS styling before the actual HTML content began. The HTML portion, containing about 40,000 lines, presented each transaction in horizontally aligned div containers where each was defined by repetitive code blocks specifying layout and styling(see the rendered image below). Then I checked for patterns in the div container HTML code. I noticed that it included transactions marked as \u201ccompleted\u201d, \u201cpending\u201d, \u201cfailed,\u201d and \u201cPersonalization within Google Pay\u201d. I marked these in my note \u2014 remove the repetitive lines and all incomplete transactions.\\n\\n![Rendered HTML Code](Rendered%20HTML%20Code.webp)\\nRendered HTML Code\\n\\n![Items marked as cancelled in the HTML file](Items%20marked%20as%20cancelled%20in%20the%20HTML%20file.webp)\\nItems marked as cancelled in the HTML file\\n\\nIn VS Code I used a code snippet from ChatGPT, which utilized Python\u2019s BeautifulSoup library to retain only the \u201ccompleted\u201d transactions, removing the rest. This process reduced about 20,000 lines of code, and then I used the Find and Replace tool to eliminate repeated lines, which reduced 12,000 lines from the file.\\n\\n![ChatGPT helping cleanup my HTML code](ChatGPT%20helping%20cleanup%20my%20HTML%20code.webp)\\nChatGPT helping cleanup my HTML code\\n\\nHowever, the HTML still has unique identifiers and other unnecessary elements, so I applied regular expressions to remove these. Let\u2019s see the details of regex patterns and other techniques I used for this cleanup.\\n\\n![Finding alphanumeric unique identifiers using RegEx in VS Code](Finding%20alphanumeric%20unique%20identifiers%20using%20RegEx%20in%20VS%20Code.webp)\\nFinding alphanumeric unique identifiers using RegEx in VS Code\\n\\n* `\\\\d{1,2}:\\\\d{2}:\\\\d{2}\\\\s*[AP]M` \u2014 For matching the time of format \u201c1:55:45 PM\u201d\\n\\n* `[\\\\w+\\\\-/\\\\?><.,]{12,}` \u2014 For matching unique identifier code of transactions, eg \u2014 \u2018d1i/lfg8SWqU31//\u2019, \u2018pWg1Hw7hTYCvKDgE\u2019, \u2018Mz6RDKtr6SmOL3rbM\u2019\\n\\n* Find `\' \'` (5 spaces) and Replace it with a single space, and repeat until this until 5 spaces become 2 spaces.\\n\\n* `\\\\n\\\\n\\\\n\\\\n\\\\n` \u2014 For matching line breaks(multiple enter/return presses).\\n\\n* `Ctrl + Shift + L` \u2014 For selecting all similar items that match the current selection.\\n\\nAfter these regexes and other techniques, I need to do a little bit of manual work for about 15 minutes, to remove some items where there are no patterns to search for. While working with regular expressions it takes many trial and error steps, so I recommend using the [regex101](https://regex101.com/) website for quick testing of patterns.\\n\\nFinally, I was left with clean text-based transaction messages, such as \u201cPaid Rs.100.00 for Play Store Recharge on 28\u201309\u20132020\u201d, but in order to convert it to a CSV format having columns Date, Details, Credit, Debit, and Balance, this text-based is not suitable. Typing or copying this data manually would be extremely time-consuming and repetitive, so I decided to leverage Llama 3, installed on my system through Ollama, for the task. _Llama, like ChatGPT\u2019s GPT Engines, is a large-language model that I can use on my local machine, allowing me to keep my transaction data private._ I accessed Llama via Windows PowerShell, spending some time crafting precise prompts to guide it in formatting the data. See our chat below.\\n\\n![Prompting Llama3 to output transactions in a specific format](Prompting%20Llama3%20to%20output%20transactions%20in%20a%20specific%20format.webp)\\nPrompting Llama3 to output transactions in a specific format\\n\\nLlama processed my requests efficiently, outputting each line in the desired format. With this structured output, I could save it as a text file, and then import it into Excel as a CSV by specifying the comma as a delimiter. Once in Excel, I applied formulas to automate the categorization of Credit and Debit transactions using the IF formula. For example, if cell A1 contained \u201cpaid,\u201d I directed the amount to the Debit column; otherwise, it went to the Credit column. I then used the formula `Previous Balance - Debit + Credit` to calculate balances, while a simple TEXT formula extracted the month and year from each date (e.g., TEXT(A1, \u201cMMMM\u201d) for the month and \u201cYYYY\u201d for the year).\\nThis approach significantly sped up the process, enabling me to format all my data within minutes.\\n\\n![Old Transactions data from Canara Bank in Excel with different formulas applied](Old%20Transactions%20data%20from%20Canara%20Bank%20in%20Excel%20with%20different%20formulas%20applied.webp)\\nOld Transactions data from Canara Bank in Excel with different formulas applied\\n\\n### What Led to the Failure of Old Transaction Data Collection?\\n\\nI simply won\u2019t categorize the transaction data collection as a complete failure; rather, I set it aside after converting it to CSV format. There\u2019s still considerable work required to prepare the data for purposes such as Dashboard building, Machine Learning, and Deep Learning. Preparing the data to the required form consumes a significant amount of time, especially when I need to focus on other important tasks, such as organizing the Axis Bank data, setting up the dashboard, and automating the data collection using Gmail, Zapier, and Google Sheets.\\n\\n![Two main problems \u2014 Date has leading spaces and lack of particulars in deatials column](Two%20main%20problems%20\u2014%20Date%20has%20leading%20spaces%20and%20lack%20of%20particulars%20in%20deatials%20column.webp)\\nTwo main problems \u2014 Date has leading spaces and lack of particulars in deatials column\\n\\nSince I converted the old bank data into CSV format and saved it, I decided to hold the works temporarily. Even though this step remains incomplete, it has been immensely valuable. It taught me the unusual power of regular expressions, illustrated how even simple ideas can lead to significant outcomes(use of \\\\n\\\\n\\\\n as RegEx), and strengthened the importance of testing our thoughts for better results.\\n\\n---\\n\\n### 4. Arranging and Cleaning Axis Bank Transaction Data\\n\\nI began by locating the two CSV files containing my Axis Bank transactions, combining and loading those files as a single dataset, and transforming the data by removing unwanted columns that come while combining the CSV files (source information and table details, for example). Once I had a clean dataset, I explored it, noting that it contained five headers: Date, Details, Credit, Debit, and Balance, with a total of 942 transactions.\\n\\nTo enhance the dataset I added two new columns to separate the data into months and years. I achieved this using the TEXT function, as explained in the HTML to CSV conversion section. Next, I turned my attention to the details column, utilizing regular expressions to remove unnecessary parts using the pandas library of Python, you can read a similar work [here](https://medium.com/@ajaytshaju/data-driven-car-buying-how-i-analyzed-the-best-options-using-python-3e3a15c5f8eb).\\n\\n## Transforming the Transaction Data\\n\\nAfter documenting the findings, I turned back to transform the bank data to add a new column labeled \u201cCategory\u201d next to the details, complete with a dropdown list for easy selection.\\n\\n![Snapshot of transformed data](Snapshot%20of%20transformed%20data.webp)\\nSnapshot of transformed data\\n\\nHowever, a significant challenge arose when it came to categorizing the transactions based on the details column. With 942 rows to process, the task of selecting a category from a list of over 70+ options became repetitive and time-consuming. I started searching for different methods to streamline the category selection process, which I will explain in the next section.\\n\\nNow I am concluding Part 1 here. If you are interested in knowing how I tackled certain challenges, fly to Part 2 of this story(read some sneak-peek of Part 2\u2014 Filling 940+ dropdowns in just 4\u20135h, integrating multiple apps to automate future data collection, and setting the dashboard)."},{"id":"reading-habit-tracker-google-sheets","metadata":{"permalink":"/blog/reading-habit-tracker-google-sheets","source":"@site/blog/2024-11-06-Step_by_Step_Guide_for_Creating_a_Reading_Habit_Tracker_in_Google_Sheets/story.md","title":"Step by Step Guide for Creating a Reading Habit Tracker in Google Sheets","description":"Read on Medium","date":"2024-11-06T00:00:00.000Z","tags":[{"inline":false,"label":"Google Sheets","permalink":"/blog/tags/google-sheets","description":"Google Sheets tag description"}],"readingTime":14.42,"hasTruncateMarker":true,"authors":[{"name":"Ajay T Shaju","title":"AI & DS Engineer","url":"https://004ajay.github.io/","page":{"permalink":"/blog/authors/ajay"},"socials":{"linkedin":"https://www.linkedin.com/in/ajay-t-shaju/","github":"https://github.com/004ajay","medium":"https://medium.com/@ajaytshaju","x":"https://x.com/004ajayt"},"imageURL":"https://github.com/004ajay.png","key":"ajay"}],"frontMatter":{"slug":"reading-habit-tracker-google-sheets","title":"Step by Step Guide for Creating a Reading Habit Tracker in Google Sheets","authors":["ajay"],"tags":["google-sheets"]},"unlisted":false,"prevItem":{"title":"Converting Bank Statements to Insights: Automating Future Data Collection using Gmail Zapier and Google Sheets Part 1","permalink":"/blog/statement-to-insights-part1"},"nextItem":{"title":"My Journey with Books: From Curiosity to Lifelong Learning","permalink":"/blog/books-blog"}},"content":"Read on [Medium](https://medium.com/@ajaytshaju/step-by-step-guide-for-creating-a-reading-habit-tracker-in-google-sheets-96920411282f)\\n\\nTracking your reading habits is essential to staying consistent and motivated in your reading journey. While many readers prefer using physical notebooks or apps to log their progress, I found a personalized Google Sheet to be the perfect tool as a tracker, because we can heavily customize it, integrate other tools, and access it anywhere on the Earth if you have a device with internet and your Gmail account signed in. In this story, I will walk you through the steps to create a reading habit tracker using Google Sheets and the data I collected over the years.\\n\\n\x3c!-- truncate --\x3e\\n\\n_Just a minute_, I wrote another story about \\"[How Reading Changed My Life](https://medium.com/@ajaytshaju/my-journey-with-books-from-curiosity-to-lifelong-learning-e3388b1d10da)\\" (click to read it). This story continues one of the sections (Reading Habit Tracker: A Work in Progress) of that story.\\n\\n## Let\'s Start\\n\\n### 1. Setting Up the Google Sheet\\n\\nI started by adding the required sheets to an empty spreadsheet on Google Sheets. I have four of them: Dashboard, Reading, Finished, and Lists.\\n\\n![4 Sheets I used to track my reading](4%20Sheets%20I%20used%20to%20track%20my%20reading.webp)\\n4 Sheets I used to track my reading\\n\\n* The _Dashboard_ contains some visualization as Scorecard, Doughnut and Bar charts, and two Pivot Tables.\\n\\n* The _Reading_ sheet contains the current books I am reading and there are 8 columns, they are Serial number, Book Title, Genre, Reading Material Type, Pages Read, Total Pages, Completion Status, and Status(Reading, Archived, Paused, and Read Later).\\n\\n* The _Finished_ sheet contains the books that I have finished reading, there are 5 columns similar to the Reading sheet, the Serial number, Book Title, Genre, Reading Material Type, and Total Pages.\\n\\n* The _List_ sheet contains the contents for drop-down for Genre and Reading Material Type. We can append more items to these lists as required in the future, rather than editing the drop-down list from its settings. The reason for moving the list to a separate sheet will be explained later.\\n\\n![Snapshot of all sheets in my Google Sheet File](Snapshot%20of%20all%20sheets%20in%20my%20Google%20Sheet%20File.webp)\\nSnapshot of all sheets in my Google Sheet File\\n\\nThe above image show my sheets, the first image is the Dashboard, the second is the reading list, the third on the left bottom corner is the finished books list, and the fourth on the bottom right corner is my Lists sheet.\\n\\n---\\n\\n### 2. Setting the structure and adding the data\\n\\nStarting with the **Lists sheet**, I manually entered some Genres and Reading Material Type and I can add as many items to these lists as I continue classifying the books and they will automatically appear in the dropdown list, given I selected the ample amount of rows which giving the data range of dropdowns.\\n\\n_Tip: See the image above if you want to see the outputs while reading these paragraphs._\\n\\nNext setting up the **Reading Sheet**, the first column is _Serial Number_ I typed 1, 2, and 3 and selected these 3 numbers, and dragged up to 11 to get the numbers automatically on my sheet. Next is _Book Title_ I manually entered each book I hold currently.\\n\\nThe next one is the _Genre_ column for which I have to input a dropdown for selecting the genre for each book. To do it\\n\\n* Select the range up to where you need the dropdowns.\\n\\n* Click on the Insert button in the Menu tab and click Dropdown.\\nSuddenly a chip-like dropdown will appear on the selected range, and the Data Validation Rules settings will appear on the right side of your Google Sheets.\\n\\n* In the Data Validation Rules, you can see a Criteria setting option, it\u2019s also a dropdown. Expand the dropdown and select the \u2018Dropdown (from a range)\u2019 option.\\n\\n* Now you will see an empty box just below the Criteria settings, there is a window icon to the right side of the empty box, click to select the data range from where you want to fetch the option for your dropdown (in my case, the Lists sheet)\\n\\n* In addition, you can select colors for each of the options, which appear on the left side of each of the options on the Data Valuation Rules setups.\\n\\n* To commit the changes press the Done button and select each genre corresponding to the book title from the dropdown list.\\n\\nThe same set of steps were done for the Reading Materials Type and Status columns, I just added color for the dropdown entries.\\n\\nFor completing the Pages Read and Total Pages I have to manually enter the values, and the Pages Read are updated once a week. The next one is the Completion Status column which contains a bar chart in red color showing a visual cue of completion, rather than seeing the page numbers. It is done using a formula called Sparkline in Google Sheets.\\n\\n`=SPARKLINE(pages_read, {\\"charttype\\",\\"bar\\";\\"max\\",total_pages;\\"color1\\",if(pages_read<total_pages, \\"red\\", \\"green\\")})`\\n\\n* pages_read: cell reference to the data point that the chart will represent.\\n\\n* `{\u201ccharttype\u201d,\u201dbar\u201d;\u201dmax\u201d,total_read;\u201dcolor1\\", if(pages_read<total_read, \u201cred\u201d, \u201cgreen\u201d)}`: This is an array of options that customize the chart:\\ncharttype, bar: Specifies that the chart type should be a bar chart.\\n\\n* max, total_read: Sets the maximum value for the chart\u2019s scale, using the value in cell total_read.\\n\\n* color1, `if(pages_read<total_read, \u201cred\u201d, \u201cgreen\u201d)`: This part determines the color of the bar:\\n  \\n  * If the value in pages_read is less than the value in total_read, the bar will be red, else bar will be green.\\n\\nThe entries on the **Finished Sheet** were similar to the Reading sheet, the only difference is the count of books is more than 50. I have the book title and total pages as text separated by a hyphen in my Google Keep Notes(for example, You Can Win \u2014 315), I copied the note to a column in the Finished Sheet and named the column Book Title. Then split the text using the **\u2018Split Text to Columns\u2019** option under the Data tab of the Menu bar with the separator (delimiter) as a hyphen, it will detect the separator automatically or we can select the delimiter as a comma, semicolon, period, space, or custom. After that, I just need to rearrange the columns. The only tiring process for both the Reading and Finished Sheets was the manual selection of genre and reading material type from the dropdown, if you have any suggestions to do it faster please comment.\\n\\n---\\n\\n### 3. Making the Dashboard\\n\\nA dashboard is the place where our data gets its shape, most managers look into some dashboards to make decisions that shape the future of a company. My dashboard is simple and intuitive as I need to get the idea with a single look rather than scrutinizing the charts. The type of visualizations used were Scorecard charts, which display a number in big letters just like a scorecard in games, a Donut chart or a Pie chart with a hole, Bar charts both vertical and horizontal, Pivot tables, and slicers. And these slicers will change the charts based on our selection, just like a filter.\\n\\nNow let\u2019s move on to the setting of each chart.\\n\\nFirst, to insert a chart, you press the Insert button on the Menu bar and click on the Charts, or you can access the same button on the right side of the Quick Access toolbar. When Inserting a chart, a dialog box called Chart Editor pops up on the right side of Google Sheets. In the Setup tab of the Chart Editor you need to select the Chart Type from the dropdown list, there are many types of charts Line, Area, Column, Pie, Scatter, Map, and Other.\\n\\n![My Dashboard](My%20Dashboard.webp)\\nMy Dashboard\\n\\nTo add the first three charts (**the scorecard charts**), you press the Scorecard Chart under Other Options. When you click on this Scorecard Chart, you need to add a key value, for that select the data range and aggregate option (in my case it is the count of Book Title in the Finished sheet). The same goes for the \u2018Total Pages Read\u2019 and \u2018Total Pages (Reading Now)\u2019, only difference is the data range. For Total Pages Read, the data range is the Total Pages column of my finished sheet, and aggregate it by SUM. And \u2018Total Pages (Reading Now)\u2019 is actually calculated in a cell underneath the scorecard, the formula I used is `=SUM(Finished!E:E, ReadingE2:E11)` sum of the finished sheet\u2019s Total Pages and Reading sheet\u2019s Pages Read.\\n\\nFor adding more charts, _you should have a clear understanding or a plan in your mind about what visualizations you need to see and from which columns_. In my mind, I want to see the visualizations of my book type, Reading Materials Type, and Current Reading Status, in different visualizations like pie charts, bar charts, etc.\\n\\nFirst, add the **Book Type to a Pie chart**. Add a new chart and select the chart type as pie. Then select the data range by clicking the window icon on the data range, drag, and select the data range(in my case, genre in Finished sheet). It will appear as a pie chart automatically, if it doesn\u2019t appear or you selected the whole data, then you can select the label from a list of column names and aggregate the value if needed. When I see it as a pie chart, I think it would be much better if I put a hole in the middle of that pie chart, that is to make it a donut chart. I selected the chart type as a donut, and the additional settings were checking \u2018Use of row 1 as headers\u2019 in the setup tab, legend as labeled on the customize tab, and the donut hole size as 75% in the pie chart option of the customize tab.\\n\\nThe next one is the **Reading Material Type**. For that, I added a column chart and selected the data range as Reading Material Type from the Finished sheet. On the X-Axis option, it automatically appeared as Reading Material Type and I checked the aggregate box to see the visualization. As additional settings, I checked \u2018use row 1 as header\u2019, changed the font size and some formatting also displayed the count inside the bars by checking the data labels option in the Series dropdown in the customize tab of the chart editor. Similar to this, I added the **Current Reading Status** visualization with horizontal bars, with the data range Status column from the Reading sheet.\\n\\n![3 Charts in My Dashboard](3%20Charts%20in%20My%20Dashboard.webp)\\n3 Charts in My Dashboard\\n\\nNext, I need to have two **Pivot Tables** in my dashboard to see the Currently Reading Book List and All books with Slicers applied. _The Pivot Tables help us see our data from another view with aggregations including sum, average, count, etc. I opted for the pivot table because it automatically adds or deletes content whenever the data in the reference changes._\\n\\nTo create a pivot table, go to the Insert option in the Menu bar and click Pivot Table, then a small dialog box named \u2018Create Pivot Table\u2019 will appear. You need to select the data range and the sheet to which the table is to be inserted (New or Existing sheet).\\n\\n![Create Pivot Table dialog box](Create%20Pivot%20Table%20dialog%20box.webp)\\n\\nCreate Pivot Table dialog box\\n\\nFor my small pivot table, I selected the whole Reading sheet in the Data range and the Existing sheet radio button to add the table to my Dashboard sheet. With this, a skeleton of the pivot table will appear in the pivot table options, now we need to add rows, columns, values, and filters to the table to get the new view. I added Book Title in the rows as I need to see the book titles as the row value, Pages Read in values as I need to see how many pages I read for each book, and finally Status as filter to filter reading status only to view books with status Reading.\\n\\n![\u2018Currently reading books\u2019 pivot table functionality](\u2018Currently%20reading%20books\u2019%20pivot%20table%20functionality.webp)\\n\'Currently reading books\' pivot table functionality\\n\\nFor the big pivot table, I did the same process for adding the table. Selected the whole data of the Finished sheet added the Serial Number and Book Title in the row and kept columns, values, and filters empty because I need to add a slicer for this dashboard to see different views based on some filters. So I added two slicers(_for adding a slicer, click the Data button on the Menu bar and click Add a slicer, then as a rounded rectangle the slicer will appear on your sheet_), the data range was column C and D for the genre and reading material type slicers respectively from the Finished sheet. Now I can select the required option in genres like Science to see the books related to that genre, and when we have some options in the slicers the other visualizations will also change. See the image below.\\n\\n![3 Options selected in slicer, and corresponding change in other charts](3%20Options%20selected%20in%20slicer,%20and%20corresponding%20change%20in%20other%20charts.webp)\\n3 Options selected in slicer, and corresponding change in other charts\\n\\n---\\n\\n### 4. Additional Features\\n\\nBy this time the tracker is somewhat ready, but I found a missing feature. Whenever I finish reading a book I need to move the book from the Reading sheet to the Finished sheet manually. So I planned to automate this task using a simple Google App Script. See the code below\\n\\n```javascript\\n// App Script code to automate moving completed books\\nfunction onEdit(e) {\\n  var sheet = e.source.getActiveSheet();\\n  var editedRange = e.range;\\n  var ReadingBooksSheet = \\"Reading\\";\\n  var FinishedBooksSheet = \\"Finished\\";\\n\\n  if (sheet.getName() === ReadingBooksSheet) {\\n    var row = editedRange.getRow();\\n    // Reading Status in Column H (8th column)\\n    var Status = sheet.getRange(row, 8).getValue();\\n    \\n    // If progress stage is marked as \\"Finished\\"\\n    if (Status === \\"Finished\\") {\\n      var rowData = sheet.getRange(row, 1, 1, 7).getValues()[0];\\n      // Use \'sheet.getLastColumn()\' to get full row\\n      \\n      var targetSheet = e.source.getSheetByName(FinishedBooksSheet);\\n      targetSheet.appendRow(rowData); // Move the row to the Finished Sheet\\n      \\n      sheet.deleteRow(row); // Delete the row from the Reading sheet\\n    }\\n  }\\n}\\n```\\n\\nThis code when saved and run will throw an error, but don\u2019t worry see the explanation from [Mr Shane on Google Docs Editors Help](https://support.google.com/docs/thread/254176258/getting-the-error-typeerror-cannot-read-properties-of-undefined-reading-source?hl=en).\\n\\n![App script error](App%20script%20error.webp)\\nApp script error\\n\\nNow with this code applied, whenever I change the Status of a book in the Reading sheet, within a second, the row will be moved to the Finished sheet. See how it works on my Google sheet\\n\\n![Working of Google App script to move finished book](Working%20of%20Google%20App%20script%20to%20move%20finished%20book.gif)\\nWorking of Google App script to move finished book\\n\\nLet\'s explore the code.\\n\\n`function onEdit(e)` defines a function `onEdit` that will run automatically whenever a user edits a cell in the Google Sheet. The parameter `e` is an event object that contains information about the edit event. The `e.source.getActiveSheet()` gets the currently active sheet (the one being edited) and `e.range` gets the range of cells that were edited (where the edit occurred). Next, I defined the variable names for Sheets, and `if (sheet.getName() === ReadingBooksSheet)` checks if the active sheet (the one being edited) is the Reading sheet, if it is then `editedRange.getRow()` gets the row number of the edited cell, with this row number we find the status using `sheet.getRange(row, 8).getValue()` where 8 is the column H on which Status of the reading progress exist.\\n\\nIf the Status is Finished, then the data in that row is taken using `sheet.getRange(row, 1, 1, 7).getValues()[0]` here\\n\\n* row: the starting row\\n\\n* 1: the starting column, A\\n\\n* 1: number of rows to return, only 1\\n\\n* 7: the number of columns to return, from A to F.\\n\\n* The `getValues()[0]` is used to get the first row of data from the returning two-dimensional array.\\n\\nThe `targetSheet.append(rowData)` appends the data to the end of the Finished sheet as a new row and `sheet.deleteRow(row)` deletes the current row from the Reading sheet. Here this `deleteRow` deletes the complete row, and I have placed the list for dropdowns like Genre and Reading Material Type near these rows, when the `deleteRow()` deletes the item in the row, it will delete an entry of the genre list too, thereby bringing problem for the genre dropdown, see the image.\\n\\n![Red mark near Autobiography indicates an error](Red%20mark%20near%20Autobiography%20indicates%20an%20error.webp)\\nRed mark near Autobiography indicates an error\\n\\n## Conclusion\\n\\nThe beauty of using Google Sheets is the **real-time update** feature. As soon as I add a book to the reading list, the charts and the pivot tables get updated automatically. When I finish a book or change its reading status to Finished, the app script will move the row to Finished sheet, effectively saving me a lot of time. With this tracker, I can review the data in equal intervals to track my progress and adjust my reading habits as needed."},{"id":"books-blog","metadata":{"permalink":"/blog/books-blog","source":"@site/blog/2024-11-06-My_Journey_with_Books_From_Curiosity_to_Lifelong_Learning/story.md","title":"My Journey with Books: From Curiosity to Lifelong Learning","description":"Read on Medium","date":"2024-11-06T00:00:00.000Z","tags":[{"inline":false,"label":"Books","permalink":"/blog/tags/books","description":"Books tag description"}],"readingTime":7.22,"hasTruncateMarker":true,"authors":[{"name":"Ajay T Shaju","title":"AI & DS Engineer","url":"https://004ajay.github.io/","page":{"permalink":"/blog/authors/ajay"},"socials":{"linkedin":"https://www.linkedin.com/in/ajay-t-shaju/","github":"https://github.com/004ajay","medium":"https://medium.com/@ajaytshaju","x":"https://x.com/004ajayt"},"imageURL":"https://github.com/004ajay.png","key":"ajay"}],"frontMatter":{"slug":"books-blog","title":"My Journey with Books: From Curiosity to Lifelong Learning","authors":["ajay"],"tags":["books"]},"unlisted":false,"prevItem":{"title":"Step by Step Guide for Creating a Reading Habit Tracker in Google Sheets","permalink":"/blog/reading-habit-tracker-google-sheets"},"nextItem":{"title":"Data-Driven Car Buying: How I Analyzed the Best Options Using Python","permalink":"/blog/data-driven-car-buying"}},"content":"Read on [Medium](https://medium.com/@ajaytshaju/my-journey-with-books-from-curiosity-to-lifelong-learning-e3388b1d10da)\\n\\nGrowing up, books weren\u2019t just items on my shelf; they were the torches I used to explore worlds, ideas I hadn\u2019t imagined, and skills I didn\u2019t even know existed. Each book brought something unique to my life, from childhood curiosity to shaping my career in Artificial Intelligence. Through books, I not only gained knowledge but also learned more about myself and the world around me. Here\u2019s my journey with reading and the impact it has had on my life, thoughts, and ambitions.\\n\\n\x3c!-- truncate --\x3e\\n\\n![Some of My Book Collections](Some%20of%20My%20Book%20Collections.webp)\\nSome of My Book Collections\\n\\n## Childhood Curiosity and the Spark of Knowledge\\n\\nAccording to my mother, my reading journey started at the age of 4 with some children-level Malayalam books. From there onwards, I used to see images and try to understand things by reading less, which helps me now watch foreign movies without English subtitles. By this time most children will be drawn to reading newspapers but I haven\u2019t, till now. It is because of the negativity spread by the local publications in their first 2\u20133 pages, if they can switch it with achievements, information, or even advertisements, then most youngsters like me would have read it. but it\u2019s business and they have to sell more, the current situation as explained above may be their selling strategy.\\n\\nThen came a time when I started searching for facts to feed my curiosity. There I was introduced to many books like Important Days, How to Read and Speak English, World Leaders, The Children\u2019s Encyclopedia, The Atlas, World Leaders, and Illustrated Oxford Dictionary(which I recommend you to buy for your children or nephews) this may weigh 2kg(4.5lbs) and may cost around Rs.2000 (24 USD) but it can give a lifetime knowledge in 1000 big pages.\\n\\nWhen I was in the 7th grade, our school introduced the **Tell Me Why** series as an option for students, most of my friends thought it was a waste of money, but this series of books by Manorama Publications played an important role in my life. Despite the skepticism around me, I asked my mother if I could buy them, and she supported my interest. I collected over 15 books from the series, though I still have only 12 with me today. These weren\u2019t just books to me \u2014 they were my private knowledge hub(first little library). I quickly read each publication, re-reading them while eagerly waiting for the next. These books introduced me to new topics, including a term that would later become my passion: Artificial Intelligence. When I read about the potential of computers and mobile devices to think like humans, it was revolutionary. Keep in mind, at that time my parents had mobile phones \u2014 without touchscreens, so the idea of machines with human-like intelligence was a thrilling mystery to me. At that time, I didn\u2019t realize this curiosity would later lead me to pursue a BTech degree in **Artificial Intelligence and Data Science**.\\n\\n\\n## The Turning Point: Self-Help and Setting a Reading Structure\\n\\nIn 12th grade, I faced a reading milestone that completely changed how I approached books. A close friend lent me a copy of **You Can Win by Shiv Khera**. I wasn\u2019t keen on self-help books at the time \u2014 I just tried _The Power of Your Subconscious Mind_. So, I initially refused to read this book. However, my friend asked me to try the first chapter, Importance of Attitude: Building a Positive Attitude \u2014 and to my surprise, I was hooked and finished the whole book within a week.\\n\\nTo return the favor, I lent her **A Brief History of Time by Stephen Hawking**. She had been hoping to find this book at the school library, but it was unavailable. This exchange sparked something in me, marking the beginning of a structured approach to my reading. I started noting down the books I read and the total number of pages, a habit I\u2019ve maintained since.\\n\\n## Discovering the Impact of Books on My Life and Thinking\\n\\nFrom that point on, my reading became a journey of self-discovery and learning. I\u2019ve read over 50 books to date, which might not seem like a large number, but each book has left a lasting impact on me.\\n\\nFor example, **Rich Dad Poor Dad by Robert Kiyosaki** entirely shifted my perspective on money, savings, and investments. **Start With Why by Simon Sinek** taught me the importance of finding a purpose in everything I do. Before reading it, I often focused on \u201cwhat\u201d and \u201chow\u201d first. But Sinek\u2019s idea of starting with \u201cwhy\u201d influenced the way I structured whatever I do from then on. **Hooked by Nir Eyal**, expanded my understanding of marketing and creating impactful products that connect deeply with users. **Atomic Habits by James Clear** taught me the power of small steps. It emphasized that big achievements come from the accumulation of consistent tiny actions. For instance, just as small amounts of water gradually form an ocean, little habits build significant change over time.\\n\\n## Expanding My Reading Horizons\\n\\nI don\u2019t limit myself to specific genres. Everything from Science to business, and even Malayalam literature \u2014 each genre has something unique to offer. Malayalam books, particularly short stories by Joseph Annamkutty Jose, inspire me by bringing out beauty in life\u2019s smallest details.\\n\\nI also explore science-psychology books like **The Brain by David Eagleman**, which offers insights into the most complex and powerful organ that ever existed, the Brain. Self-help classics such as **How to Win Friends and Influence People and How to Stop Worrying and Start Living by Dale Carnegie** have taught me the value of networking, public speaking, and maintaining healthy relationships for personal and professional growth.\\n\\nTo complement my studies, I\u2019ve also been reading books on Artificial Intelligence. **Life 3.0** by Max Tegmark, which I\u2019m currently reading, has been enlightening on AI\u2019s impact on humanity and offers a broad perspective on what it means to be human in an AI-driven world.\\n\\n## Reading Habit Tracker: A Work in Progress\\n\\nTo further enrich my reading journey, I\u2019m currently working on building a **Reading Habit Tracker using Google Sheets**. Once completed, this tracker will help me record and provide insights about the books I have read. In addition, I am planning to share a detailed technical look at this setup in a future blog post. So stay tuned for the update.\\n\\n## How Reading Has Transformed My Perspective\\n\\nReading has profoundly impacted my way of thinking and personal growth. Here are a few valuable lessons I\u2019ve learned that might resonate with other readers:\\n\\n1. **Understanding at Your Own Pace**: When you read regularly and grasp what you\u2019re reading, it fine-tunes your ability to understand things at your own pace. This \u201cspeed of understanding\u201d helps me recognize whether I\u2019m gaining value from content or simply consuming it mindlessly \u2014 whether it\u2019s a social media post or an informative video.\\n\\n2. **Recognizing Supporting Ideas**: Often, when I read, I find myself connecting the material with personal experiences or similar situations. This helps to reinforce the memory and meaning behind what I\u2019m reading. For instance, while reading about the concept of \u201cvariable rewards\u201d in Hooked, I instantly thought of how YouTube Shorts and Instagram Reels use this technique to keep users engaged.\\n\\n3. **Analyzing Perspectives and Biases**: Books have also taught me to recognize other people\u2019s perspectives and biases. This skill helps me better understand others in conversations and interactions, whether in real life or online. Distinguishing between my own biases and others\u2019 viewpoints has made me more adaptable and open-minded.\\n\\n4. **Asking Questions**: One of the most valuable habits I\u2019ve developed from reading is the ability to ask insightful questions. Reading **Rich Dad Poor Dad** made me curious about Ray Kroc and his impact on McDonald\u2019s. What if he hadn\u2019t seized the opportunity to standardize the brand? What if he\u2019d remained a paper cup salesman? Such questions deepen my understanding and strengthen my critical thinking.\\n\\n## My Lifelong Reading Goal\\n\\nI don\u2019t see books as just collections of pages; _they are like lifetimes of experience compressed into a few hundred pages_. My goal is to read widely and learn from diverse topics, never confining myself to a set of genres. Books are a way for me to expand my knowledge, broaden my perspective, and continue growing as a person. Who knows where the next book will lead me? But one thing is certain: the habit of reading has shaped who I am, and it will continue to guide me toward the person I aspire to become."},{"id":"data-driven-car-buying","metadata":{"permalink":"/blog/data-driven-car-buying","source":"@site/blog/2024-10-21-Data_Driven_Car_Buying_How_I_Analyzed_the_Best_Options_Using_Python/story.md","title":"Data-Driven Car Buying: How I Analyzed the Best Options Using Python","description":"Read on Medium","date":"2024-10-21T00:00:00.000Z","tags":[{"inline":false,"label":"Data","permalink":"/blog/tags/data","description":"Data tag description"},{"inline":false,"label":"Python","permalink":"/blog/tags/python","description":"Python tag description"}],"readingTime":9.7,"hasTruncateMarker":true,"authors":[{"name":"Ajay T Shaju","title":"AI & DS Engineer","url":"https://004ajay.github.io/","page":{"permalink":"/blog/authors/ajay"},"socials":{"linkedin":"https://www.linkedin.com/in/ajay-t-shaju/","github":"https://github.com/004ajay","medium":"https://medium.com/@ajaytshaju","x":"https://x.com/004ajayt"},"imageURL":"https://github.com/004ajay.png","key":"ajay"}],"frontMatter":{"slug":"data-driven-car-buying","title":"Data-Driven Car Buying: How I Analyzed the Best Options Using Python","authors":["ajay"],"tags":["data","python"]},"unlisted":false,"prevItem":{"title":"My Journey with Books: From Curiosity to Lifelong Learning","permalink":"/blog/books-blog"},"nextItem":{"title":"How to use TensorFlow with GPU on Windows for Heavy Tasks (2024)","permalink":"/blog/tf-heavy"}},"content":"Read on [Medium](https://medium.com/@ajaytshaju/data-driven-car-buying-how-i-analyzed-the-best-options-using-python-3e3a15c5f8eb)\\n\\n![Car Data Analysis to find best car for my needs](Car%20Data%20Analysis%20to%20find%20best%20car%20for%20my%20needs,%20(Image%20by%20Author,%20Python%20Logo%20from%20Wikipedia).webp)\\nCar Data Analysis to find best car for my needs, (Image by Author, Python Logo from Wikipedia)\\n\\n## The backstory\\n\\nSince the start of 2024, there has been a discussion in my home regarding purchasing a new car, but the talks and planning have gone on without a final decision. But when my current car started costing a good amount just from routine checkups and other services, the talk rose again to full power. Just like every time, it was my responsibility to find the latest and best car based on some constraints, but this time I thought of analyzing an Indian Cars dataset (get the data [here](https://www.kaggle.com/datasets/atharvataras/indian-car-market-2024?select=india_cars_2024.csv)) using Python rather than searching on Google and filtering through websites.\\n\\n\x3c!-- truncate --\x3e\\n\\n**The first question in your mind: Why did he choose Python Analysis over applying some Filters on Car Websites?**\\n\\nThe Google search and filtering functions on car websites like CarDekho or CarWale are much easier but I choose Python:\\n\\n* Most importantly, by engaging with the dataset directly, I was able to customize my search parameters in a way that web filters might not allow. I could explore a wider range of variables, perform complex comparisons, and apply unique criteria to my specific needs.\\n\\n* I aimed for a more hands-on, analytical approach to enhance my understanding of data analysis with Python and Excel. As well as finding something extra(maybe a new car that I didn\u2019t find initially) through this analysis, where the probability is slim in the usual web searches.\\n\\n* And, if the decision is not finalized, when more data arrives I can use the updated data and run through this notebook again to find the next best car for my needs. Sometimes a reader like you can make use of my [Python Notebook](https://github.com/004Ajay/Data-Analysis/blob/main/Car%20EDA%20Python/car_selection.ipynb) for your specific car-searching needs as well.\\n\\n## Let\u2019s Jump onto the work\\n\\nSee a snapshot of the data in MS Excel 2019:\\n\\n![A bird-eye view of the data](A%20bird-eye%20view%20of%20the%20data%20(Image%20by%20Author).webp)\\nA bird-eye view of the data (Image by Author)\\n\\n***Some information***: The coding and structure of the blog remain consistent throughout. I used the [Pandas](https://pandas.pydata.org/docs/index.html) library in IPython in VS Code to perform this Exploratory Data Analysis(EDA). I followed four steps for this analysis, they are Loading and Viewing the data; Removing unnecessary columns and rows; Comparison with my current car, Honda Amaze 2015.\\n\\nFirst of all, we need to bring the data to IPython VS Code, for that use the pandas function `read_csv()`\\n\\n```python\\nimport pandas as pd\\ncars = pd.read_csv(\'india_cars_2024.csv\', encoding=\'utf-8\')\\n```\\n\\nBut here comes the first problem\\n\\n![The first error in Pandas](The%20first%20error%20in%20Pandas%20(Image%20by%20Author).webp)\\nThe first error in Pandas (Image by Author)\\n\\nJust change the encoding to unicode_escape, found from here\\n\\n```python\\nimport pandas as pd\\ncars = pd.read_csv(\'india_cars_2024.csv\', encoding=\'unicode_escape\')\\n```\\n\\nOutput:\\n\\n![View of Data in Pandas](View%20of%20Data%20in%20Pandas%20(Image%20by%20Author).webp)\\nView of Data in Pandas (Image by Author)\\n\\nWhile viewing the data, I peeked at the dimensionality, which includes 1,663 rows and 57 columns. Many of these columns and rows were not relevant to my analysis, so I decided to make the dataset lean for faster processing and better absorption of info.\\n\\nI then viewed the dataset\'s columns using the `dataset.columns` attribute.\\n\\nThen to check for null values in each column, I used `dataset.isnull().sum().sort_values(ascending=False)`, which gave me an idea about null values in the DataFrame. Sorted it in descending order, allowing me to identify columns with no entries(if any).\\n\\n```python\\nprint(cars.columns)\\n\\ncars.isnull().sum().sort_values(ascending=False).head()\\n\\n# Index([\'brand_parent\', \'model_parent\', \'variant_parent\', \'variant_name\',\\n#       \'price\', \'displacement\', \'power\', \'transmission\', \'mileage\', \'fuel\',\\n#       \'engine_typr\', \'cylinders\', \'valves_per_cyl\', \'gearbox\', \'drive\',\\n#       \'fuel_cap\', \'emission_norm\', \'suspension_front\', \'suspension_rear\',\\n#       \'steer_type\', \'steer_col\', \'turn_radius\', \'brake_front\', \'breake_rear\',\\n#       \'length\', \'width\', \'height\', \'boot_cap\', \'seat_cap\', \'ground_cl\',\\n#       \'wheelbase\', \'kerb_weight\', \'gross_weight\', \'doors\', \'park_sensors\',\\n#       \'upholstery\', \'radio_antenna\', \'tyre_size\', \'tyre_type\', \'wheel_size\',\\n#       \'airbags\', \'screen_size\', \'connectivity\', \'image-src\', \'ncap_rating\',\\n#       \'ev_range\', \'ev_battery_cap\', \'ev_motor\', \'ev_charge\', \'ev_drag_coeff\',\\n#       \'zero_to_hundred\', \'speakers\', \'auto_park\', \'ev_charge_time_dc\',\\n#       \'ev_charge_time_ac\', \'ev_brake_regen_levels\', \'ev_norm\'],\\n#       dtype=\'object\')\\n```\\n\\n## Removing Unwanted Columns\\n\\nNext, I removed unwanted columns to slim down the dataset. This is important because removing columns reduces the dataset\u2019s dimensions, thus enabling faster data processing. Before removing these columns, I explored all 57 columns using specific functions (`<column>.unique()`, `<column>.isnull()`) to check their contents, identify missing values, and find unique values. Some constraints I had were that I do not prefer Electric or CNG vehicles, as electric vehicles could increase electricity charges(homes without Solar Panels) and CNG vehicles typically have lower boot space, acceleration performance, horsepower, resale value, and high maintenance costs. In order to remove the \u2018Electric\u2019 columns I wrote code to find columns starting with \u2018ev\u2019 (which stands for Electric Vehicle) and identified about nine such columns and dropped them.\\n\\n```python\\nev_cols = [column for column in cars if column.startswith(\'ev\')]\\n\\nprint(ev_cols)\\n\\n# Output: [\'ev_range\',\\n# \'ev_battery_cap\',\\n# \'ev_motor\',\\n# \'ev_charge\',\\n# \'ev_drag_coeff\',\\n# \'ev_charge_time_dc\',\\n# \'ev_charge_time_ac\',\\n# \'ev_brake_regen_levels\',\\n# \'ev_norm\']\\n\\ncars.drop(ev_cols, axis=1, inplace=True) # axis=1 means columns\\n```\\n\\nWhile exploring the dataset, I noticed some columns contained irrelevant information, such as \u201cradio antenna,\u201d also identified other columns having redundant info, like \u201cbrand_parent,\u201d \u201cmodel_parent,\u201d \u201cvariant_parent,\u201d and \u201cvariant_name.\u201d The \u201cvariant_name\u201d contained information from the other three columns, so I removed them. Similar to this, there exist many other columns that provide useful information, but I removed them as I would later look for relevant details when analyzing the final car list.\\n\\nWhen I reviewed the dataset, I encountered a significant issue with the dataset: the presence of SI units alongside the values(row entries). For example, in the mileage column, a car might have a mileage of \u201c19 km per liter,\u201d which is not suitable for integer-level comparisons. Thus, I needed to change the entries from  `<value><SI unit>` to just `value` and convert it to an integer, and rename the column as `<column name>_<SI unit>`.\\n\\nI updated several such columns, including mileage, gross weight, displacement, fuel capacity, boot capacity, length, width, height, and ground clearance, filled empty or NA values with 0, and changed the data type to integer. To extract the numbers, I used a regular expression `r\'\\\\d+\'`, to find integers within the strings. And dropped the original columns to avoid redundancy. Codes:\\n\\n```python\\ncars[\'gross_weight_kg\'] = cars[\'gross_weight\'] \\\\\\n                                      .str.extract(r\'(\\\\d+)\') \\\\\\n                                      .fillna(0) \\\\\\n                                      .astype(int) \\n\\ncars[\'mileage_kmpl\'] = cars[\'mileage\'] \\\\\\n                                      .str.extract(r\'(\\\\d+)\') \\\\\\n                                      .fillna(0) \\\\\\n                                      .astype(int)\\n# Output:\\n#        mileage_kmpl\\n#          12\\n#          17\\n```\\n\\n```python\\nto_drop_cols = [\'mileage\', \'gross_weight\', \'displacement\',\\n                \'fuel_cap\', \'boot_cap\', \'length\', \'width\',\\n                \'height\', \'ground_cl\']\\n\\ncars.drop(to_drop_cols, axis=1, inplace=True)\\n```\\n\\n## Removing Unwanted Rows\\n\\nIn the next step, I removed unwanted rows from the dataset. I began by removing cars with fuel types \u201cElectric\u201d and \u201cCNG\u201d using a filtering function.\\n\\n```python\\ncars = cars[~cars[\'fuel\'].isin([\'Electric\', \'CNG\'])]\\n```\\n\\nNext, I needed to address the price column, which presented another challenge. The prices were listed in crores or lakhs, multiplied by \u201con-road price\u201d and \u201cex-showroom price,\u201d with the only delimiter being an asterisk (*). To resolve this, I used the `split(\'*\')` function to separate these values with an asterisk as a delimiter and extracted the first entry, representing the price value.\\n\\n```python\\n# Before: \'Rs.8.07 Lakh*Get On-Road Price*Ex-showroom Price\'\\n# After: \'Rs.8.07 Lakh\' \\n\\ncars[\'price\'] = cars[\'price\'].apply(lambda x: str(x).split(\'*\')[0])\\n\\ncars[\'price\'].head()\\n\\n# Output\\n#    Rs.68.90 Lakh\\n#    Rs.68.25 Lakh\\n#       Rs.1.01 Cr\\n#       Rs.6.95 Cr\\n#      Rs.10.48 Cr\\n```\\n\\nAfter cleaning the price data, I found that the price column entries included the string \u201cCr\u201d (for crores) in various cases. To filter out these entries, I used the `str.contains()` method with `case=False`, which allowed me to remove approximately 211 rows containing crore-priced cars.\\n\\n```python\\ncrore_priced_cars = cars[\'price\'].str.contains(\'cr\', case=False, na=False)\\n```\\n\\nNext, I need to identify two keywords, \\"Rs.\\" and \\"Lakh,\\" in the price column, these followed a pattern (Rs.9.9 Lakh). So I removed those words, converted the remaining values to a float, renamed the new column as \u2018price_lakhs\u2019, and finally dropped the original price column using a series of steps.\\n\\n```python\\npattern = r\\"rs.|(\\\\s\\\\w+)\\" # using regex pattern to find multiple strings,\\n                         # \'rs.\' is a word with fullstop,\\n                         # \'\\\\s\' finds any whitespace character\\n                         # \'\\\\w+\' a complete word that follows\\n\\n# Changing \'Rs.62.95 Lakh\' like values to 62.95 (float) \\nlakh_priced_cars = cars[\'price\'].str \\\\\\n                               .lower() \\\\\\n                               .replace(pattern, \\"\\", regex=True) \\\\\\n                               .astype(float) \\n\\ncars[\'price_lakhs\'] = lakh_priced_cars # Adding new column \'price_lakhs\'\\n\\ncars.drop(\'price\', axis=1, inplace=True) # Dropping the \'price\' column\\n```\\n\\n## Comparison with Current Car\\n\\nThis is the most important part of my car-finding process. When planning to buy a new vehicle, we all hope to upgrade from our existing one. This is the same in my case too, find a better car than my 2015 Honda Amaze.\\n\\nOne significant consideration is the narrow path to my home, which limits the size of vehicles that can be accommodated. The largest car that can fit is the 2024 Maruti Suzuki Brezza, with dimensions of 3,995 mm in length, 1,790 mm in width, and 1,685 mm in height. Considering these dimensions, I set a criterion that any car less than or equal to these measurements would be acceptable.\\n\\nAdditionally, the Amaze has an engine displacement of 1.2 liters, so any car with an engine displacement greater than or equal to this is acceptable. The price range I set was between 8 to 10 lakhs. Furthermore, I required the mileage to be greater than 12 kmpl. I also added the boot capacity requirement; although the Amaze has a boot capacity of 400 liters, I decided to accept cars with a minimum boot capacity of 300 liters. Finally, I combined these criteria and filtered the dataset accordingly.\\n\\n```python\\n# Conditions combined to filter out rows\\n\\ncars = cars[\\n    (cars[\'price_lakhs\'] >= 8.00)     &\\n\\n    (cars[\'price_lakhs\'] <= 10.00)    & \\n\\n    (cars[\'mileage_kmpl\'] >= 12)      &\\n    \\n    (cars[\'length_mm\'] <= 3995)       &\\n    \\n    (cars[\'width_mm\'] <= 1790)        &\\n    \\n    (cars[\'height_mm\'] <= 1685)       &\\n\\n    (cars[\'boot_cap_liters\'] >= 308)\\n]\\n```\\n\\nAfter applying these combined conditions, the DataFrame has a dimension of 112 rows and 24 columns. Then to see the shortlisted cars, one of the columns titled \u2018variant_name\u2019 has been extracted, but in that column, I found multiple entries for a car like \u2018Citroen C3 Shine Dual Tone Turbo\u2019, \u2018Citroen C3 Feel Dual Tone Turbo\u2019, \u2018Nissan Magnite Turbo XV Premium Opt\u2019, \u2018Nissan Magnite Turbo XV Premium DT\u2019. So I did a set of operations to find the unique ones.\\n\\n```python\\nprint(\\"Final Cars: \\", cars[\'variant_name\'] \\\\\\n                             .apply(lambda x: \' \'.join(x.split()[:2])) \\\\\\n                             .unique()\\n)\\n\\n# Output:\\n# Final Cars: [\'Citroen C3\' \'Nissan Magnite\' \'Renault Kiger\' \'Honda Amaze\'\\n#           \'Hyundai i20\' \'Hyundai Aura\' \'Hyundai Exter\' \'Hyundai Venue\'\\n#           \'Toyota Taisor\' \'Tata Tigor\' \'Tata Altroz\' \'Tata Punch\'\\n#           \'Maruti Dzire\' \'Maruti Baleno\' \'Maruti FRONX\' \'Maruti Brezza\']\\n```\\n\\nThis resulted in a total of 16 distinct cars. Now I can show this car list to my stakeholders, and they as well as I can proceed to the examination of each of these cars, looking into service charges, aftermarket costs, modifications, and other criteria.\\n\\nFinally, I exported the required data as a CSV file for later use.\\n\\nSee the complete Notebook and other materials [here on my GitHub](https://github.com/004Ajay/Data-Analysis/tree/main/Car%20EDA%20Python).\\n\\nThat\u2019s all about \'Data-Driven Car Buying Using Python\', if you liked the post please clap and share to support my work. Thanks \ud83d\ude03"},{"id":"tf-heavy","metadata":{"permalink":"/blog/tf-heavy","source":"@site/blog/2024-05-07-How_to_use_TensorFlow_with_GPU_on_Windows_for_Heavy_Tasks_2024/story.md","title":"How to use TensorFlow with GPU on Windows for Heavy Tasks (2024)","description":"Read on Medium","date":"2024-05-07T00:00:00.000Z","tags":[{"inline":false,"label":"TensorFlow","permalink":"/blog/tags/tensorflow","description":"TensorFlow tag description"},{"inline":false,"label":"GPU","permalink":"/blog/tags/gpu","description":"GPU tag description"}],"readingTime":12.66,"hasTruncateMarker":true,"authors":[{"name":"Ajay T Shaju","title":"AI & DS Engineer","url":"https://004ajay.github.io/","page":{"permalink":"/blog/authors/ajay"},"socials":{"linkedin":"https://www.linkedin.com/in/ajay-t-shaju/","github":"https://github.com/004ajay","medium":"https://medium.com/@ajaytshaju","x":"https://x.com/004ajayt"},"imageURL":"https://github.com/004ajay.png","key":"ajay"}],"frontMatter":{"slug":"tf-heavy","title":"How to use TensorFlow with GPU on Windows for Heavy Tasks (2024)","authors":["ajay"],"tags":["tensorflow","gpu"]},"unlisted":false,"prevItem":{"title":"Data-Driven Car Buying: How I Analyzed the Best Options Using Python","permalink":"/blog/data-driven-car-buying"},"nextItem":{"title":"How to make a Virtual Environment in Python (Windows)","permalink":"/blog/venv-python"}},"content":"Read on [Medium](https://medium.com/@ajaytshaju/how-to-use-tensorflow-with-gpu-on-windows-for-heavy-tasks-2024-a1536448b0fe)\\n\\nIn the last blog [\u201cHow to use TensorFlow with GPU on Windows for minimal tasks \u2014 in the most simple way(2024)\u201d](https://medium.com/@ajaytshaju/how-to-use-gpu-with-tensorflow-on-windows-b1726a28ab3d) I discussed how to use Microsoft TensorFlow-DirectML Plugin to make GPU available to TensorFlow(TF) in Windows, it is good for lightweight programming tasks that requiring GPU, but not good for programs that take too much graphics memory and requires heavy loading(as these frameworks need drivers and applications like CUDA and cuDNN to allocate memory for processing the data and task). So in this blog, we are going to deal with downloading and installing the correct versions of TensorFlow, CUDA, cuDNN, Visual Studio Integration, and other driver files to make GPU accessible to TensorFlow, for doing heavy tasks like Video Processing, High-Resolution Image Processing, and Deep Learning*.\\n\\n\x3c!-- truncate --\x3e\\n\\n* _These tasks are heavily dependent on your GPU type and VRAM available._\\n\\nBe informed:\\n\\n* The last version of TensorFlow that supports GPU on Windows is TF-2.10 (it supports most of the functionalities but it\u2019s good to use the latest and stable versions).\\n\\n* This tutorial is mainly for Windows 11 users(as most Win11 will have hardware with a supported compute version for Nvidia Apps and Drivers).\\n\\n* All these downloads (especially the Visual Studio integration items) will take a good amount of secondary memory, so make sure you have about 30GB(approximate) of free memory on your computer.\\n\\n* This blog will be an easy-to-understand walkthrough of directions given in TensorFlow Website and is intended for those who know only TensorFlow or like to code in TensorFlow. There is another deep learning framework named PyTorch, which is also an easy-to-use with support for GPU.\\n\\n![TF Caution shown on TF Website](TF%20Caution%20shown%20on%20TF%20Website.webp)\\nTF Caution shown on TF Website\\n\\nNow we are going to do the _\u201cInitial Checks, Download TensorFlow, Cleanup, Downloading other items, Installing the downloaded items one by one, Adding to Path, Testing, and we are done\u2026\u201d_\\n\\n## Processes\\n\\n### 1. Initial Checks\\n\\nTo make sure you have a physical GPU in our system that can be detected by your system, use the command `wmic path win32_videocontroller get caption` in your command prompt. Open the command prompt, Click Windows Icons \u2192 type \u2018cmd\u2019 and open the command prompt.\\n\\n![List of all available GPUs in your system](List%20of%20all%20available%20GPUs%20in%20your%20system.webp)\\nList of all available GPUs in your system\\n\\nIf this command is giving an error, check if your device manager is listing the physical GPU by, Right click on the Windows icon \u2192 device manager \u2192 drop-down Display Adapters. If GPU is not listed, see other drop-downs and see other option on the internet.\\n\\n### 2. Downloading TensorFlow\\n\\nYou will need `TensorFlow 2.10` for this, and you can download it using `pip install tensorflow==2.10`. _Read the below paragraph before sudden action._\\n\\nIf you have gone through my [last blog](https://medium.com/@ajaytshaju/how-to-use-gpu-with-tensorflow-on-windows-b1726a28ab3d) to make TF work with GPU, then you will have TF CPU 2.10 and other plugins as well, but here we need normal TF 2.10. You can either start a venv(refer to [this](https://medium.com/@ajaytshaju/how-to-make-a-virtual-environment-in-python-windows-17a30b67d3bc)) or you can install TF for the user. I suggest starting with venv because it doesn\u2019t interfere with other libraries and their dependencies while installation, also if the steps explained in this blog have completely worked out for you, you just need to take a little time to install TF for the user(if you want TF to use GPU wherever you create code files in your computer, without the need of activating a venv).\\n\\nIf you are installing TF for the user, there is an additional step. Go to cmd, Click Windows Icons \u2192 type \u2018cmd\u2019 and open the command prompt, now type `pip list | findstr tensorflow`. To find package names with string \u2018tensorflow\u2019. You have to uninstall all packages listed using the pip command above. Also to make sure there are no other TF components, use `pip list` to list all the packages and manually go through the list to find any TF items, if you find any, uninstall it using the name shown in the list (`pip uninstall <package-name>`). Finally, install TF using `pip install tensorflow==2.10` and you will get something like\\n\\n![List of TF packages](List%20of%20TF%20packages%20after%20installing%20TF(I%20have%20installed%20for%20the%20user).webp)\\nList of TF packages after installing TF(I have installed for the user)\\n\\n### 3. Cleanup/Uninstalling\\n\\nHere we do everything fresh, but you can opt for resuming from any kind of installation you have done, else if you are ready to start from scratch then go to your computer\u2019s Settings \u2192 Apps \u2192 Installed Apps \u2192 Search \u2018Nvidia\u2019.\\n\\n![Installed Nvidia Apps](Installed%20Nvidia%20Apps.webp)\\nInstalled Nvidia Apps\\n\\nYou can see something like this on your application list, you can uninstall every one of it safely, except the Nvidia Control Panel. If you haven\u2019t installed any Nvidia components this list might be empty. _These uninstalls may take some time and multiple restarts, don\u2019t worry._\\n\\n### 4. Downloading Other Items\\n\\nAfter all the uninstalls and a fresh restart, you are good to go for downloading all the required files one by one. This hack will help you install them faster. While this process may not go in a linear fashion, it will save a whole lot of time. _We\u2019re trying parallel downloading here._\\n\\nFirst lets start with the downloading of Nvidia latest driver for your GPU and GeForce Experience App. For this go to [this site](https://www.nvidia.com/en-us/drivers/), and you will see something like\\n\\n![Nvidia Driver Search](Nvidia%20Driver%20Search.webp)\\nNvidia Driver Search\\n\\n![Resultant Driver after giving specifications](Resultant%20Driver%20after%20giving%20specifications.webp)\\nResultant Driver after giving specifications\\n\\nDownload this driver and keep it in your downloads folder we will install it later. While Nvidia Driver is downloading, move to download the CUDA toolkit from [this site](https://developer.nvidia.com/cuda-11-8-0-download-archive?target_os=Windows&target_arch=x86_64&target_version=11&target_type=exe_network).\\n\\n![CUDA 11.8 Download Website](CUDA%2011.8%20Download%20Website.webp)\\nCUDA 11.8 Download Website\\n\\nIn the _Installer Type_, you have two options \u2018local\u2019 or \u2018network\u2019. Local means you have to download a big package of about 3-4GB and do not need an internet connection for installation. In the Network installer, your initial download size is only 30MB but during installation, it will download and install the 3-4GB of CUDA toolkits from the Internet, so you should have a stable internet connection during installation. However, the subsequent installation steps are the same for both installer types.\\n\\nWhile CUDA is downloading, move to download cuDNN on [this website](https://developer.nvidia.com/rdp/cudnn-archive#a-collapse860-118). Scroll down and click on cuDNN v8.6.0, see the image below.\\n\\n![cuDNN Installer Page](cuDNN%20Installer%20Page(download%20the%20windows%20zip).webp)\\ncuDNN Installer Page(download the windows zip)\\n\\nWhile cuDNN is downloading, move to download Visual Studio on [this](https://visualstudio.microsoft.com/downloads/) website, and download the community version of Visual Studio(this is a small-sized download). In addition to Visual Studio, you have to download the Microsoft Visual C++ Redistributable Versions 2015\u20132022 from [this site](https://learn.microsoft.com/en-us/cpp/windows/latest-supported-vc-redist?view=msvc-170#latest-microsoft-visual-c-redistributable-version).\\n\\n![Visual Studio Community Installer](Visual%20Studio%20Community%20Installer.webp)\\nVisual Studio Community Installer\\n\\n![Download Microsoft Visual C++ Redistributable as per your System Architecture](Download%20Microsoft%20Visual%20C++%20Redistributable%20as%20per%20your%20System%20Architecture.webp)\\n\\nDownload [Microsoft Visual C++ Redistributable](https://learn.microsoft.com/en-us/cpp/windows/latest-supported-vc-redist?view=msvc-170#latest-microsoft-visual-c-redistributable-version) as per your System Architecture\\n\\nAfter downloading all the items you will have something like the below image in your downloads folder. With this many items, our downloading is done.\\n\\n![Downloaded Items](Downloaded%20Items.webp)\\nDownloaded Items\\n\\n### 5. Installation\\n\\nInstallation should go in the order, VS Redistributable(lightest installation) \u2192 VS Individual Components \u2192 GeForce Experience(Nvidia Driver) \u2192 CUDA Toolkit 11.8 \u2192 cuDNN (copy-paste). _Please don\u2019t try for parallel installation._\\n\\n* Visual Studio Redistributable Installation: This is a minimal installation you just double-click on the VS Redistributable icon (4th one in the above image) and follow their on-screen prompts. Sometimes you may knowingly or unknowingly installed the VS Redistributables before, in this case, you need to \u2018modify\u2019 it and install it again.\\n\\n* Visual Studio Components Installation: Even though you may not use Visual Studio as your default code editor, this is necessary for CUDA to work with Windows as CUDA doesn\'t come with its own C++ compiler, but it assumes that the compiler is installed in the system. So we need to install the C++ compiler from another vendor (here, Microsoft Visual Studio). Visual Studio contains all the necessary components, ex: the C++ compiler toolchain for Windows. This is one of the parts where the installation of CUDA gets very messy in Windows. See the image below to know what all items need to be installed from Visual Studio.\\n\\n\\n![Vis studio install1](List%20of%20items%20that%20need%20to%20be%20installed%20from%20Visual%20Studio2.webp)\\n![Vis studio install2](List%20of%20items%20that%20need%20to%20be%20installed%20from%20Visual%20Studio1.webp)\\n\\nList of items that need to be installed from Visual Studio\\n\\nGeForce Experience(Nvidia Driver) Installation: (2nd one in the \u2018Downloaded Image\u2019) When you have uninstalled all the Nvidia components, the commands like `nvidia-smi` will not work and you may not have the GeForce Experience application. Both of these problems will be corrected by installing the previously downloaded Nvidia driver. This installation is also minimal, you just need to follow the on-screen prompts and mark any checkbox for installing the GeForce Experience application and then install. After these installation you can check if commands like `nvidia-smi` is working or not in you command prompt. The output will be like\\n\\n![Output of nvidia-smi command](Output%20of%20\u2018nvidia-smi\u2019%20command.webp)\\nOutput of \u2018nvidia-smi\u2019 command\\n\\n* CUDA Toolkit Installation: (3rd one in the \u2018Downloaded Image\u2019) The installation procedures are given on the left side of the image below. Agree, accept, continue on every prompt and under the installation part do the \u2018Express Installation\u2019. During any of this installation part, there will be checking for Visual Studio Integration, if that checking fails, then you have to check the installation of C++ compilers and other components from Visual Studio. When all the installation is finished(will take some time) you have successfully installed CUDA Toolkits on your computer, this is also one of the toughest parts of this whole installation. Also, this installation may bring new items like Nsight Compute and other IDE-like items that we won\u2019t use, but they are important in this installation and the proper working of CUDA.\\n\\n\\n![Nvidia sw license](Nvidia_license.webp)\\n\\n![Nvidia Installer](Nvidia_Installation.webp)\\n\\nThis CUDA installation can be confirmed using the command `nvcc --version` on your command prompt. It will return something like the image below, else you will get an error like `\'nvcc\' is not recognized as an internal or external command, operable program or batch file.` Check the installation once more.\\n\\n![CUDA installation checking command](CUDA%20installation%20checking%20command.webp)\\nCUDA installation checking command\\n\\nAfter the CUDA installation, extract the cuDNN Zip(1st one in the \u2018Downloaded Image\u2019) and open two file explorers, in one explorer open the CUDA bin, include, and lib, and on the other open the cuDNN bin, include, and lib. See the path to the CUDA folder in the image below.\\n\\n![CUDA bin, include and lib](CUDA%20bin%20include%20and%20lib.webp)\\nCUDA bin include and lib\\n\\n![cuDNN bin, include and lib](cuDNN%20bin,%20include%20and%20lib.webp)\\ncuDNN bin, include and lib\\n\\nYou need to move the contents of cuDNN to CUDA\u2019s corresponding folders(bin, include, and lib). These are some .dll, .h, and .lib files required for the cuDNN to work properly on your computer, while you run deep learning programs. _Opening the folders side by side will help you move the files faster across these folders._\\n\\n![Opening CUDA and cuDNN folders side by side](Opening%20CUDA%20and%20cuDNN%20folders%20side%20by%20side.webp)\\nOpening CUDA and cuDNN folders side by side\\n\\n### 6. Adding to Path\\n\\nActually, during the installation, the path will be added automatically so we don\u2019t need to add the path manually but we need to check whether the paths are added correctly or not.\\n\\nCheck Paths: Click the Windows icon \u2192 type \u2018env\u2019 \u2192 Edit the system environment variables(Enter) \u2192 Environment Variables(bottom-right button) \u2192 System Variable \u2192 check CUDA_PATH, CUDA_PATH_V11_8, and double click on \u2018Path\u2019 check for CUDA items in the list.\\n\\n![CUDA_PATH in Environment Variables](CUDA_PATH%20in%20Environment%20Variables.webp)\\nCUDA_PATH in Environment Variables\\n\\n![Path of CUDA in Environment Variables](Path%20of%20CUDA%20in%20Environment%20Variables.webp)\\nPath of CUDA in Environment Variables\\n\\n### 7. Testing\\n\\nPaste the below command in your cmd, if you are using venv, then activate the venv and paste this command(getting output by running this command will take some time).\\n\\n```python\\npython -c \\"import tensorflow as tf; print(tf.config.list_physical_devices(\'GPU\'))\\"\\n```\\n\\nIf all the installations were correct, then you will get an output like the image below, else most probably you will get an empty list [].\\n\\n![Checking whether TensorFlow can find our GPU](Checking%20whether%20TensorFlow%20can%20find%20our%20GPU.webp)\\nChecking whether TensorFlow can find our GPU\\n\\nIf the output is an empty list, try doing\\n\\n* Restart your system, the new components and paths may need a restart.\\n\\n* Installed version of TensorFlow (is it 2.10), check this by `pip show tensorflow` command.\\n\\n* If you are using venv, make sure the venv is active. This can be confirmed by checking your command prompt `(your_venv_name)c:\\\\Users\\\\<username>\\\\<venv_location> > <command here>` or check this blog.\\n\\n* `nvcc --version` in the command prompt, if you forgot to check it while installing CUDA.\\n\\n* Recheck the versions of CUDA and cuDNN you have installed, it is 11.8 for CUDA and 8.6 for cuDNN, a click-mistake will change everything.\\n\\n* Check `nvidia-smi` if it is not giving a box-like output, then the Nvidia Driver installation is not correct.\\n\\n* Search for more ways to debug on the internet.\\n\\n### 8. Extras\\n\\nIf you have followed the tutorial and got the correct output up to the 7th step, you have configured CUDA and cuDNN correctly and TensorFlow can find your GPU successfully. But running some kind of code like [this one](https://github.com/004Ajay/Main-Project/blob/main/Main/train.py) (my BTech Main Project) may introduce other errors like missing .dll, .h, or .lib files. To solve this problem you have to search online to get that specific file, and put it in the directory where the error is pointing. In my case it was a .dll file named `zlibwapi.dll`, and pointed to the CUDA Bin directory, then I needed to add the specific file to the path.\\n\\n![System env vars](sys_env_var.webp)\\n\\n![Edit env vars](env_var_edit.webp)\\n\\nThank you for following up till the very end. Hope you can burn your GPU to its limit, happy coding."},{"id":"venv-python","metadata":{"permalink":"/blog/venv-python","source":"@site/blog/2024-03-29-How_to_make_a_Virtual_Environment_in_Python_Windows/story.md","title":"How to make a Virtual Environment in Python (Windows)","description":"Read on Medium","date":"2024-03-29T00:00:00.000Z","tags":[{"inline":false,"label":"Windows","permalink":"/blog/tags/windows","description":"Windows tag description"},{"inline":false,"label":"Virtual Environments","permalink":"/blog/tags/venv","description":"Virtual Environments tag description"},{"inline":false,"label":"Python","permalink":"/blog/tags/python","description":"Python tag description"}],"readingTime":2.15,"hasTruncateMarker":true,"authors":[{"name":"Ajay T Shaju","title":"AI & DS Engineer","url":"https://004ajay.github.io/","page":{"permalink":"/blog/authors/ajay"},"socials":{"linkedin":"https://www.linkedin.com/in/ajay-t-shaju/","github":"https://github.com/004ajay","medium":"https://medium.com/@ajaytshaju","x":"https://x.com/004ajayt"},"imageURL":"https://github.com/004ajay.png","key":"ajay"}],"frontMatter":{"slug":"venv-python","title":"How to make a Virtual Environment in Python (Windows)","authors":["ajay"],"tags":["windows","venv","python"]},"unlisted":false,"prevItem":{"title":"How to use TensorFlow with GPU on Windows for Heavy Tasks (2024)","permalink":"/blog/tf-heavy"},"nextItem":{"title":"How to use TensorFlow with GPU on Windows for minimal tasks\u2014 in the most simple way(2024)","permalink":"/blog/tf-minimal"}},"content":"Read on [Medium](https://medium.com/@ajaytshaju/how-to-make-a-virtual-environment-in-python-windows-17a30b67d3bc)\\n\\nPython\u2019s Virtual Environment(venv) is a self-contained directory tree that contains a Python installation for a particular version of Python, with several additional packages.\\n\\n\x3c!-- truncate --\x3e\\n\\nIt\u2019s uses:\\n\\n* Manage dependencies for different projects by creating isolated spaces.\\n* Maintain different versions of Python libraries and Python itself.\\n* Separate requirements (python version and library versions) of each project, without conflicts.\\n\\n> _Be informed:_ You can either use pip(command line) or data science packages like Anaconda to run Python in your system, here I am using pip.\\n\\n## Overview of making a venv\\n\\n1. Find a location\\n2. Open the Terminal of that folder\\n3. Typing the venv creation command\\n4. Activate the environment\\n\\n---\\n\\n### 1. Find the location\\n\\nGo to a suitable location on your computer. In this tutorial, I am making the venv in the Local Disk D.\\n\\n### 2. Open the Terminal of that folder\\n\\nYou can open the terminal either by right click \u2014 Open In Terminal or by clicking the folder path top of the folder window and typing `cmd`\\n\\n![Right Click \u2014 Open in Terminal](Right%20Click%20\u2014%20Open%20in%20Terminal.webp)\\n\\nRight Click \u2014 Open in Terminal\\n\\n![Opening command prompt using folder path tab](Opening%20command%20prompt%20using%20folder%20path%20tab.webp)\\nOpening command prompt using folder path tab\\n\\n### 3. Type this command\\n\\nOn the terminal, type `python -m venv tf_gpu` to make a venv with the name `tf_gpu`, you can use any name of your choice the syntax is `python -m venv <your_env_name>`\\n\\nA short explanation of the command:\\n\\n* `python`: The Python interpreter. You\u2019re telling your system to do something using Python. You can make venv with a specific version of Python by adding the version number, ex: `python3.10.11` but you need to have the specified version of Python installed on your system.\\n\\n* `-m`: Flag that tells Python to run a library module as a script.\\n\\n* `venv`: The module you\u2019re asking it to run. The venv module is a built-in module used for creating virtual environments in Python.\\n\\n* `tf_gpu`: This is the name of the new virtual environment you\u2019re creating.\\n\\n### 4. Activating the environment\\n\\nOn the same terminal, type\\n\\n`tf_gpu\\\\Scripts\\\\activate`\\n\\nThis will activate your venv, indicated by the venv name in brackets on the left side of the terminal command typing area.\\n\\n## Exploring the venv\\n\\nType `pip list` to see the list of installed packages. Then if you have time, update pip using the green command given. Install all required libraries and enjoy working with Python venv.\\n\\n![Complete Process](Complete%20Process.webp)\\nComplete Process"},{"id":"tf-minimal","metadata":{"permalink":"/blog/tf-minimal","source":"@site/blog/2024-03-29-How_to_use_TensorFlow_with_GPU_on_Windows_for_minimal_tasks_in_the_most_simple_way_2024/story.md","title":"How to use TensorFlow with GPU on Windows for minimal tasks\u2014 in the most simple way(2024)","description":"Read on Medium","date":"2024-03-29T00:00:00.000Z","tags":[{"inline":false,"label":"TensorFlow","permalink":"/blog/tags/tensorflow","description":"TensorFlow tag description"},{"inline":false,"label":"GPU","permalink":"/blog/tags/gpu","description":"GPU tag description"}],"readingTime":3.33,"hasTruncateMarker":true,"authors":[{"name":"Ajay T Shaju","title":"AI & DS Engineer","url":"https://004ajay.github.io/","page":{"permalink":"/blog/authors/ajay"},"socials":{"linkedin":"https://www.linkedin.com/in/ajay-t-shaju/","github":"https://github.com/004ajay","medium":"https://medium.com/@ajaytshaju","x":"https://x.com/004ajayt"},"imageURL":"https://github.com/004ajay.png","key":"ajay"}],"frontMatter":{"slug":"tf-minimal","title":"How to use TensorFlow with GPU on Windows for minimal tasks\u2014 in the most simple way(2024)","authors":["ajay"],"tags":["tensorflow","gpu"]},"unlisted":false,"prevItem":{"title":"How to make a Virtual Environment in Python (Windows)","permalink":"/blog/venv-python"},"nextItem":{"title":"Building my Deep Learning project, Emotion2Emoji: Challenges and Chuckles","permalink":"/blog/emotion2emoji"}},"content":"Read on [Medium](https://medium.com/@ajaytshaju/how-to-use-gpu-with-tensorflow-on-windows-b1726a28ab3d)\\n\\nAccelerating machine learning code using your system\u2019s GPU will make the code run much faster and save a lot of time. In this blog we are going to do a quick setup to make TensorFlow access our PC/Laptop\u2019s GPU, just like installing 3\u20134 libraries in python using pip.\\n\\n\x3c!-- truncate --\x3e\\n\\nBe informed:\\n\\n* The latest TensorFlow version is 2.16 (As of March 2024), but TensorFlow can access GPU only if its version is `<= 2.10` in Windows. So if your code is written in the latest version you might need to fix the syntax and function calls.\\n* Some commands given in this tutorial may install big libraries which are gigabytes in size so be careful about your internet.\\n\\n## TensorFlow GPU access steps overview:\\n\\n* Install Python 3.10.11\\n\\n* Update CUDA Driver to the latest version\\n\\n* Install TensorFlow-CPU 2.10\\n\\n* Install tensorflow-directml-plugin\\n\\n* Run a sample code\\n\\n---\\n\\nGood to know:\\n\\nYou can install Python libraries for the user(may result in package version conflicts, if you have multiple projects) or in a virtual environment(more recommended). I recommend you first try the installation on a virtual environment(venv) and if it works, then you can activate and use that venv interpreter whenever you need to run a code on GPU.\\n\\n---\\n\\n_I am explaining the installation procedure using venv_\\n\\n## Things to do before the real installation\\n\\n* Make a virtual environment and activate it ([see this tutorial](https://medium.com/@ajaytshaju/how-to-make-a-virtual-environment-in-python-windows-17a30b67d3bc)).\\n\\n* Open your command prompt, type `nvidia-smi` and hit enter (will need this later). If you get an error, then make sure you have a dedicated GPU and your device can find that device (checking \u2014 Start \u2192 device manager \u2192 display adapters \u2192 name of your graphics card)\\n\\n![nvidia-smi command output](nvidia-smi%20command%20output.webp)\\nnvidia-smi command output\\n\\n* Install PyTorch(for getting necessary CUDA & cuDNN libraries easily):\\n\\n    * Search `PyTorch Start Locally`\\n    \\n    * Give details like PyTorch Build, Your OS, Package(pip, conda), Language, Compute Platform(select CUDA as seen on nvidia-smi), Run the resultant command on your command prompt (see the image below).\\n\\n![PyTorch Start Locally](PyTorch%20Start%20Locally.webp)\\nPyTorch Start Locally\\n\\n## Real change-makers\\n\\n* Install Python 3.10.11\\n\\nThis is because to install tensorflow-cpu and DirectML the maximum supported version of python is 3.10\\n\\nCheck your python version by typing `python --version` on your command prompt, if you have other version of python then type `pip list` in the same cmd to list your installed python packages for the user, copy them to notepad to install necessary libraries later. Now uninstall the unsupported python version and install python 3.10, [direct-link](https://www.python.org/downloads/release/python-31011/)\\n\\n![Python website (for 64bit windows)](Python%20website%20(for%2064bit%20windows).webp)\\nPython website (for 64bit windows)\\n\\n* Update CUDA Drivers\\n\\n* Go to the NVIDIA GeForce Experience App and check for driver updates, if there is any new update then install it.\\n\\n* * If you don\u2019t have the GeForce Experience App then go to this site fill in all details, download the resultant driver, and install it with/without the GeForce Application.\\n\\n![NVIDIA GeForce \u2014 check for driver updates](NVIDIA%20GeForce%20\u2014%20check%20for%20driver%20updates.webp)\\nNVIDIA GeForce \u2014 check for driver updates\\n\\n* Install base TensorFlow library:\\n\\n    `pip install tensorflow-cpu==2.10`\\n\\n* Install tf-directml plugin\\n\\n    `pip install tensorflow-directml-plugin`\\n\\n* Check if TensorFlow can find your GPU\\n\\n```python\\nimport tensorflow as tf\\ntf.config.list_physical_devices(\'GPU\')\\n# sample output -- [PhysicalDevice(name=\'/physical_device:GPU:0\', device_type=\'GPU\'),\\n#                  PhysicalDevice(name=\'/physical_device:GPU:1\', device_type=\'GPU\')]\\n```\\n\\nIf you are getting a similar output as above, you try running a small code that utilizes GPU. Don\u2019t forget to put `with tf.device(\'/GPU:0\')` on the main function.\\n\\nIf you have a doubt about which GPU number to use, then try running with the 0 or 1 and check your task manager(ctrl+shift+esc \u2192 performance \u2192 GPU0 and GPU1) which GPU is getting utilized and use the number accordingly."},{"id":"emotion2emoji","metadata":{"permalink":"/blog/emotion2emoji","source":"@site/blog/2024-02-21-Building_my_Deep_Learning_project_Emotion2Emoji_Challenges_and_Chuckles/story.md","title":"Building my Deep Learning project, Emotion2Emoji: Challenges and Chuckles","description":"Read on Medium","date":"2024-02-21T00:00:00.000Z","tags":[{"inline":false,"label":"Python","permalink":"/blog/tags/python","description":"Python tag description"},{"inline":false,"label":"Deep Learning","permalink":"/blog/tags/deep-learning","description":"Deep Learning tag description"}],"readingTime":3.58,"hasTruncateMarker":true,"authors":[{"name":"Ajay T Shaju","title":"AI & DS Engineer","url":"https://004ajay.github.io/","page":{"permalink":"/blog/authors/ajay"},"socials":{"linkedin":"https://www.linkedin.com/in/ajay-t-shaju/","github":"https://github.com/004ajay","medium":"https://medium.com/@ajaytshaju","x":"https://x.com/004ajayt"},"imageURL":"https://github.com/004ajay.png","key":"ajay"}],"frontMatter":{"slug":"emotion2emoji","title":"Building my Deep Learning project, Emotion2Emoji: Challenges and Chuckles","authors":["ajay"],"tags":["python","deep-learning"]},"unlisted":false,"prevItem":{"title":"How to use TensorFlow with GPU on Windows for minimal tasks\u2014 in the most simple way(2024)","permalink":"/blog/tf-minimal"},"nextItem":{"title":"Windows and Ubuntu Side by Side: Easy Solutions for Common Dual Boot Issues in High-End Laptops","permalink":"/blog/dual-booting-windows-ubuntu"}},"content":"Read on [Medium](https://medium.com/@ajaytshaju/building-my-deep-learning-project-emotion2emoji-challenges-and-chuckles-dd2abb18e173)\\n\\nIn the dynamic landscape of technology, innovation becomes a beacon that guides creators to captivate audiences with unique and engaging projects. For me, this journey began with a challenge from my college\u2019s technical fest, Asthra 8.0. The head of the department tasked me with crafting something special, something that would not only capture attention but also evoke joy. Thus, the inception of Emotion2Emoji \u2014 a lightweight system that predicts and displays emojis from facial emotions in a live webcam feed.\\n\\n\x3c!-- truncate --\x3e\\n\\n![Emotion2Emoji](Snapshot%20of%20the%20creator%20doing%20the%20demonstration%20of%20Emotion2Emoji.webp)\\nSnapshot of the creator doing the demonstration of Emotion2Emoji (image by author)\\n\\n## Why Emotion2Emoji\\n\\nThe core inspiration behind Emotion2Emoji stemmed from the desire to create an interactive and entertaining experience. As a technology enthusiast, I often find myself delving into complex projects, but this time, the aim was to bring a smile to every face that encountered this innovation. The idea evolved from mere facial expression(emotion) recognition to dynamically predicting and displaying emojis in real-time, adding an element of fun to the user experience.\\n\\n## Overcoming Challenges\\n\\nEmbarking on this project, I faced several challenges that tested my skills and perseverance. One significant hurdle was my limited experience with computer vision projects involving live webcam feeds. Unlike previous projects, Emotion2Emoji required real-time processing of facial expressions, emotion prediction, and emoji display \u2014 all seamlessly integrated into a single OpenCV window.\\n\\n```python\\n# Previously I tried OpenCV for this\\n# type of simple task, a CV-Camera\\n\\nimport cv2\\n\\ncap = cv2.VideoCapture(0)\\nimage_count = 1\\n\\nwhile True:\\n    ret, frame = cap.read()\\n    cv2.imshow(\\"Webcam Feed\\", frame)\\n    key = cv2.waitKey(1)\\n    \\n    if key == ord(\'s\'):\\n        cv2.imwrite(f\\"{image_count}.jpg\\", frame)\\n        print(\\"Image captured\\")\\n        image_count += 1\\n\\n    elif key == 27:\\n        break\\n\\ncap.release()\\ncv2.destroyAllWindows()\\n```\\n\\nThe quest for a reliable deep learning library capable of accurate facial emotion detection was another challenge. After experimenting with various open-source projects, I discovered the gem \u2014 DeepFace. This library not only provided stable emotion predictions but also became the backbone of Emotion2Emoji, ensuring the system\u2019s success.\\n\\nThe final obstacle lay in the presentation \u2014 displaying emojis as it is, without a background. Internet searches yielded no straightforward solution. Blending alpha channels became a daunting task, and there came the helpers: ChatGPT and Google Gemini they suggested some ways to solve the problem and it worked. However, overcoming these challenges not only enhanced the project\u2019s functionality but also enriched my skill set.\\n\\nUsing artificial intelligence in our projects is important because it\u2019s like the next big thing after regular internet searches. But, we shouldn\u2019t totally depend on it. It\u2019s more about finding the right balance. We can make our ideas better by using these AI tools, but we also need to ask the right questions to get the best results. So, it\u2019s about using AI to help us, not taking over our creativity.\\n\\n> Checkout my project\u2019s GitHub page [here](https://github.com/004Ajay/Emotion2Emoji)\\n\\n## The Significance of DeepFace\\n\\nDeepFace emerged as a game-changer, seamlessly integrating into the Emotion2Emoji system. Its accuracy and stability added a layer of reliability that was crucial for the success of the project, making it an invaluable component. DeepFace has many utilities lying inside it like Age, Gender, Race detection, etc\u2026 I have used only emotion detection from DeepFace.\\n\\n## The Emotional Connect\\n\\nWhy does Emotion2Emoji hold emotional significance for me? The answer lies in the joy and amusement it brings. Crafting a system that not only works seamlessly but also elicits smiles from users is a rewarding experience. The laughter and delight that Emotion2Emoji can bring make every challenge, every late-night coding session, and every problem overcome worth it. The emotional connection lies in the shared enjoyment \u2014 both in creating and experiencing the project.\\n\\n## Conclusion\\n\\nEmotion2Emoji stands as proof of the power of innovation and perseverance. From a college fest challenge to a fully functional, entertaining system, this project represents the fusion of technology and fun. The journey from conceptualization to implementation was marked by challenges, triumphs, and a deep emotional connection to the joy it brings. As we continue to explore the vast possibilities of technology, projects like Emotion2Emoji remind us that sometimes, the most powerful innovations are the ones that make people smile."},{"id":"dual-booting-windows-ubuntu","metadata":{"permalink":"/blog/dual-booting-windows-ubuntu","source":"@site/blog/2023-08-29-Windows_and_Ubuntu_Side_by_Side_Easy_Solutions_for_Common_Dual_Boot_Issues_in_High_End_Laptops/story.md","title":"Windows and Ubuntu Side by Side: Easy Solutions for Common Dual Boot Issues in High-End Laptops","description":"Read on Medium","date":"2023-08-29T00:00:00.000Z","tags":[{"inline":false,"label":"Windows","permalink":"/blog/tags/windows","description":"Windows tag description"},{"inline":false,"label":"Ubuntu","permalink":"/blog/tags/ubuntu","description":"Ubuntu tag description"},{"inline":false,"label":"Linux","permalink":"/blog/tags/linux","description":"Linux tag description"}],"readingTime":4.67,"hasTruncateMarker":true,"authors":[{"name":"Ajay T Shaju","title":"AI & DS Engineer","url":"https://004ajay.github.io/","page":{"permalink":"/blog/authors/ajay"},"socials":{"linkedin":"https://www.linkedin.com/in/ajay-t-shaju/","github":"https://github.com/004ajay","medium":"https://medium.com/@ajaytshaju","x":"https://x.com/004ajayt"},"imageURL":"https://github.com/004ajay.png","key":"ajay"}],"frontMatter":{"slug":"dual-booting-windows-ubuntu","title":"Windows and Ubuntu Side by Side: Easy Solutions for Common Dual Boot Issues in High-End Laptops","authors":["ajay"],"tags":["windows","ubuntu","linux"]},"unlisted":false,"prevItem":{"title":"Building my Deep Learning project, Emotion2Emoji: Challenges and Chuckles","permalink":"/blog/emotion2emoji"},"nextItem":{"title":"Useful Keyboard shortcuts in Jupyter Notebook","permalink":"/blog/jupyter-notebook-keyboard-shortcuts"}},"content":"Read on [Medium](https://medium.com/@ajaytshaju/windows-and-ubuntu-side-by-side-easy-solutions-for-common-dual-boot-issues-in-high-end-laptops-bc933da65160)\\n\\nThe journey of dual-booting Windows and Ubuntu (or any other OS) on a high-end laptop can be a bit tricky. But still, it can be an exciting endeavor \u2014 challenges and problem-solving. This article presents a comprehensive guide for addressing two common issues users face while dual-booting high-end laptops: setting the storage configuration and resolving sound output problems.\\n\\n\x3c!-- truncate --\x3e\\n\\nWindows and Ubuntu (Image by author)\\n\\n> Multi-booting is the act of installing multiple operating systems on a single computer and being able to choose which one to boot. The term dual-booting refers to the common configuration of specifically two operating systems. \u2014 Wikipedia\\n\\n### Note: Prioritize Data Safety\\n\\n_Dual-booting your system may lead to permanent data loss, so take strict steps to ensure the safety of your valuable data, like backing up your important files and documents. Consider using external storage devices, cloud platforms, or any other reliable backup method to safeguard your data. I backed up all my data to an external hard disk before dual-booting the system._\\n\\nTechnical specifications of my system:\\n\\nLaptop: ASUS ROG Strix G15\\n\\nProcessor: Intel Core i7 10750H\\n\\nGraphics Card: Nvidia GTX 1660Ti\\n\\nRAM: 16GB (8GB SO-DIMM * 2) DDR4\\n\\nStorage: 1TB M.2 NVMe PCIe 3.0 SSD\\n\\nI have freed up 100GB of space for the new operating system: UBUNTU 22.04.3 LTS. And installed the new OS. After installing the new OS two problems have arisen, the storage configuration problem and the sound issue with the Ubuntu OS.\\n## Challenges\\n\\n### Challenge 1: Bridging the Storage Configuration Gap\\n\\nWindows supports Intel Rapid Storage Technology (RST) and Advanced Host Controller Interface (AHCI) storage configuration but my laptop was pre-configured Windows with Intel RST storage configuration. But Ubuntu works only on AHCI storage configuration, got this information from [here](https://www.python.org/downloads/release/python-31011/). This mismatch posed a major obstacle to successful dual booting. Attempting to boot Windows on AHCI configuration led to boot failures and automatic system repairs.\\n\\nTo solve this issue, I have gone through many solutions on the internet like changing the registry key in Windows (it worked for one of my friends but not for me). But at last, I got a solution from Stack Overflow (I did the dual booting weeks ago the link to that stack overflow answer is missing, may add it in the future).\\n\\nFirst, Windows was set to boot in Safe Mode, allowing the necessary configuration changes without interference. Then in the BIOS, the SATA configuration was changed from Intel RST to AHCI mode. This step was crucial for Windows and Ubuntu to operate smoothly. Then I booted back to Windows to remove the safe boot configuration and then restarted Windows and it worked normally. This sequence of steps proved to be the key to resolving the storage configuration dilemma for me. Now Windows and Ubuntu could now be booted without any technical glitches.\\n\\n![Changing SATA Configuration in ROG Laptop](Changing%20SATA%20Configuration%20in%20ROG%20Laptop.webp)\\nChanging SATA Configuration in ROG Laptop (Image by author)\\n\\nThe commands I used for safe booting and setting SATA Configuration to AHCI:\\n\\n1. Open the Windows command prompt with admin privileges.\\n\\n2. Type `bcdedit /set {current} safeboot minimal` , if this didn\u2019t work try, `bcdedit /set safeboot minimal`\\n\\n3. Restart the computer and enter BIOS Setup (the key to BIOS in my system was ESC but it varies between systems, find yours by searching it on the internet).\\n\\n4. Change the SATA Configuration mode from Intel RST to AHCI.\\n\\n5. Save changes and exit BIOS. Then, boot back to Windows (it automatically boots in Safe Mode).\\n\\n6. Windows desktop may appear to be zoomed or have any other problems in safe mode, but don\u2019t worry, open the command prompt once again with admin privileges.\\n\\n7. Type `bcdedit /deletevalue {current} safeboot` , if this didn\u2019t work try, `bcdedit /deletevalue safeboot`\\n\\n8. Reboot once more and Windows will automatically start with AHCI drivers enabled.\\n\\n### Challenge 2: No Sound in Ubuntu\\n\\nUbuntu had another problem while dual booting \u2014 no sound output. After extensive searches and other attempts like updating and reinstalling Pipewire, Alsamixer, and Pulseaudio to fix the issue, I became doubtful that Windows was somehow influencing the situation.\\n\\n![Computer Speaker](Computer%20Speaker.webp)\\nComputer Speaker (Image from pexels)\\n\\nStraightaway, I went to the BIOS once more. This time, it was to test disabling the Fast Boot option. This choice aimed to eliminate potential conflicts between the two operating systems, especially since Windows tends to optimize booting speed at the expense of compatibility. Following this, I booted into Windows and did some file cut and paste (to know if any issues persisted) then I shut the Windows and booted into Ubuntu, With a surprise the attempt worked, and the sound came to life. To enjoy this moment, I played the RRR song [Naatu-Naatu](https://www.youtube.com/watch?v=aYMrjEChVxs) on YouTube on Ubuntu.\\n\\n## Conclusion\\n\\nThe journey of dual booting the ASUS ROG Strix G15 with Windows and Ubuntu was one of the technical challenges and a deeper understanding of how these operating systems interact with the hardware. Some strategic approaches, involving BIOS configurations, Safe Mode maneuvers, and Fast Boot adjustments, have worked for me. To simplify, _a minor change would reflect a big change_, so I hope it may work for you too, If not don\'t be down, try exploring other ways \u2014 _even if all attempts fail you will learn something new_, which will help you to solve future problems."},{"id":"jupyter-notebook-keyboard-shortcuts","metadata":{"permalink":"/blog/jupyter-notebook-keyboard-shortcuts","source":"@site/blog/2023-02-19-Useful_Keyboard_shortcuts_in_Jupyter_Notebook/story.md","title":"Useful Keyboard shortcuts in Jupyter Notebook","description":"Read on Medium","date":"2023-02-19T00:00:00.000Z","tags":[{"inline":false,"label":"Jupyter Notebook","permalink":"/blog/tags/jupyter-notebook","description":"Jupyter Notebook tag description"}],"readingTime":8.05,"hasTruncateMarker":true,"authors":[{"name":"Ajay T Shaju","title":"AI & DS Engineer","url":"https://004ajay.github.io/","page":{"permalink":"/blog/authors/ajay"},"socials":{"linkedin":"https://www.linkedin.com/in/ajay-t-shaju/","github":"https://github.com/004ajay","medium":"https://medium.com/@ajaytshaju","x":"https://x.com/004ajayt"},"imageURL":"https://github.com/004ajay.png","key":"ajay"}],"frontMatter":{"slug":"jupyter-notebook-keyboard-shortcuts","title":"Useful Keyboard shortcuts in Jupyter Notebook","authors":["ajay"],"tags":["jupyter-notebook"]},"unlisted":false,"prevItem":{"title":"Windows and Ubuntu Side by Side: Easy Solutions for Common Dual Boot Issues in High-End Laptops","permalink":"/blog/dual-booting-windows-ubuntu"}},"content":"Read on [Medium](https://medium.com/@ajaytshaju/useful-keyboard-shortcuts-in-jupyter-notebook-3d488c1e5d29)\\n\\nHow to work faster in jupyter notebooks \u2014 the answer is _Keyboard shortcuts_.\\n\\nKeyboard shortcuts are a great way to do digital work faster, when it comes to programming it\u2019s more relevant. With this article, I am sharing some of my daily keyboard shortcuts while using jupyter notebook.\\n\\n\x3c!-- truncate --\x3e\\n\\nJupyter Notebook is a powerful python code editor which can be installed on your local machine and works in the browser. I have been coding with jupyter notebook for about two years now and using a keyboard shortcut from the first week itself (i like to do things without touching the mouse, it seems more professional \ud83d\ude04)\\n\\nYou can always read more about jupyter notebook on [Wikipedia](https://en.wikipedia.org/wiki/Project_Jupyter)\\n\\n![The interface of a new jupyter notebook](The%20interface%20of%20a%20new%20jupyter%20notebook.webp)\\nThe interface of a new jupyter notebook\\n\\n_Note: This article may help windows & Linux users directly, for MAC users, there will be some changes in keys \u2014 **ctrl \u2192 command, Alt \u2192 Option**_\\n\\n_When a jupyter notebook starts, we could see whole keyboard shortcuts by pressing `esc` & `h`_\\n\\nThe first thing most of us may do on jupyter notebook is to change the title but as of now, there is no default keyboard shortcut for title renaming, but I have set a custom keyboard shortcut for renaming the title\u2014 which will be explained at the next section of this article\\n\\nNow, let\u2019s jump on to excellent jupyter notebook keyboard shortcuts\\n\\n## Keyboard Shortcuts\\n\\n### 1. Escape key to exit the current cell\\n\\nWhen the notebook starts, the first cell will be active(cell edit mode) so to change the mode to command mode \u2014 simply press `esc` key\\n\\n_we could see this change by the change in color of this cell outline_\\n\\n![An active cell](An%20active%20cell.webp)\\nAn active cell\\n\\n![An inactive cell i.e command mode](An%20inactive%20cell%20ie%20command%20mode.webp)\\nAn inactive cell i.e command mode\\n\\n_These active and command modes are important, most commands given in this article work on command mode_\\n\\n### 2. Letter \u2018A\u2019 to add multiple cells above (command mode)\\n\\nWe need more cells to work on, we could add more cells by clicking the letter **A** multiple times. We could add cells below by clicking **B**, but we use A initially to make the cell selection line(cell outline) stay at the top cell. We could move up & down through cells using the **UP & DOWN** arrow keys, and **ENTER** key to enter the selected cell.\\n\\n![A notebook with multiple cells](A%20notebook%20with%20multiple%20cells.webp)\\nA notebook with multiple cells\\n\\n### 3. Autofill code using `tab` key (active mode)\\n\\nAfter entering a cell and while writing the code, it is very helpful if the code editor has a code completion like [PyCharm](https://www.jetbrains.com/pycharm/) & [VS Code](https://code.visualstudio.com/). Jupyter notebook also has this functionality in `tab` key. You just need to press the tab key wherever you need to complete the code, this could be a variable name, a function name, etc\u2026\\n\\n![Code completion](Code%20completion.webp)\\nCode completion\\n\\n### 4. Run the current cell and select the next cell using `Shift+Enter` (active mode)\\n\\nAfter entering a cell and writing some code, we need to run it right? \u2014 then press `shift+enter` to run the cell & go to the next cell.\\n\\n![A cell with an output](A%20cell%20with%20an%20output.webp)\\nA cell with an output\\n\\n### 5. To see function documentation & examples \u2014 `shift+tab` (active mode)\\n\\nIf we don\u2019t know what are the arguments & return of a function, we could find them by pressing `shift+tab`\\n\\n_Note: you need to run the import statement to get the documentation of a function_\\n\\n![Function documentation & examples](Function%20documentation%20&%20examples.webp)\\nFunction documentation & examples\\n\\n### 6. Showing Line numbers \u2014 `shift+L` (command mode)\\n\\nBy default, the line numbering is off in jupyter notebook but it is very helpful to see the line numbers, especially when our code has some errors. You can turn on the line numbers by clicking `shift+L` in command mode.\\n\\n![A cell with line numbers](A%20cell%20with%20line%20numbers.webp)\\nA cell with line numbers\\n\\n### 7. Find and replace using the \u2018F\u2019 key (command mode)\\n\\nChanging a variable name that is written in different places is a tedious task, we could do it simply using `find and replace` just like many other applications. In jupyter notebook, you just need to press the letter **F** in command mode(blue outline around the cell)\\n\\nEnter the word to find in the `Find` box and word to replace in the `Replace` box\\n\\n![Find and replace box](Find%20and%20replace%20box.webp)\\nFind and replace box\\n\\nThis \u2193 is an example usage of find and replace. The words enclosed in the red box will be replaced with words in the green box\\n\\n![Find and replace sample](Find%20and%20replace%20sample.webp)\\nFind and replace sample\\n\\n### 8. M for Markdown and Y for code (command mode)\\n\\nIf you finished writing the code and you are ready to explain or send it to your friend or professor, then it would be a good practice to add some Markdown or LaTeX commands in your notebook to explain the idea more clearly.\\n\\n_Markdown is a simple markup language to write formatted text. It will take only 1 or 2 hours to study the markdown_\\n\\n_LaTeX is a software used for document preparation, especially research papers \u2014 extra info \ud83d\ude09_\\n\\nFor this, just make a new cell above(A) or below(B) and just press the letter M to convert the cell to a markdown cell. _The Markdown cells do not have the `In[ ]` at the cell\u2019s starting._\\n\\n![A markdown cell](A%20markdown%20cell.webp)\\nA markdown cell\\n\\n![An executed markdown cell](An%20executed%20markdown%20cell.webp)\\nAn executed markdown cell\\n\\nTo convert your cell to a normal coding cell, just press the letter Y.\\n\\n![A code cell](A%20code%20cell.webp)\\nA code cell\\n\\n### 9. Delete cells using `dd` (command mode)\\n\\nIt is better to delete the unused cells for the final draft of your notebook, just press the letter `d` twice to delete the current cell. You can undo this action by pressing `Z`\\n\\n* User-Defined Keyboard Shortcuts \u2014 I have defined 2 shortcuts in my notebook, to make my workflow faster.\\n\\n---\\n\\nHow to define your own Keyboard Shortcuts in jupyter notebook?\\n\\n`Help \u2192 Edit Keyboard Shortcuts`\\n\\n![Edit Keyboard Shortcuts](Edit%20Keyboard%20Shortcuts.webp)\\n\\n## Edit Keyboard Shortcuts\\n\\n![The new keyboard shortcut for running all cells](The%20new%20keyboard%20shortcut%20for%20running%20all%20cells.webp)\\nThe new keyboard shortcut for running all cells\\n\\nHere, I am adding `Alt+Ctrl+R` a key bind for `run all cells`. Hit enter to commit the new shortcut and click `OK` button on the right bottom.\\n\\n_Make sure, your new shortcut does not exist in the notebook & if your new shortcut contains multiple keys, separate them using hyphen `-`_\\n\\n![All cells executed](All%20cells%20executed.webp)\\n\\nAll cells executed\\n\\n### 1. Renaming the notebook (command mode)\\n\\nI have added a shortcut for renaming the notebook by pressing the letter N.\\n\\n![Custom shortcut for renaming notebook](Custom%20shortcut%20for%20renaming%20notebook.webp)\\n\\nCustom shortcut for renaming notebook\\n\\n![When the letter N is pressed](When%20the%20letter%20N%20is%20pressed.webp)\\nWhen the letter N is pressed\\n\\n### 2. Move cells up and down (command mode)\\n\\nWhen we have some relevant codes in a cell, it is more faster to move the cells up or down than cut and paste.\\n\\nI have added a shortcut to move cells using `Alt+Up arrow` for moving cells up and `Alt+Down arrow` for moving cells down.\\n\\n![Before moving the cell up](Before%20moving%20the%20cell%20up.webp)\\nBefore moving the cell up\\n\\n![After moving the cell up](After%20moving%20the%20cell%20up.webp)  \\nAfter moving the cell up\\n\\n---\\n\\n**Reached till here?**\\n\\nSome extra jupyter notebook tips and tricks for you\\n\\n## Extra Tips\\n\\n### 1. Re-run your notebook for perfect results\\n\\nWhen you finish writing the code and you got the result, don\'t stop the process but do, `Kernel \u2192 Restart & Clear Output` to restart the kernel and clear all the outputs from your notebook. Then re-run all the cells by `Cell \u2192 Run All`.\\n\\n_By doing this, the notebook will clear all the saved variables from its environment and redo all the processes again, it may be sometimes computationally intensive but give you the perfect results._\\n\\n### 2. Presenting the notebook\\n\\nPresenting the notebook separately in a tab of your browser will make you stand out. To do this, activate the presentation mode `view \u2192 cell toolbar \u2192 slideshow`\\n\\nIt will open up a drop-down at the right side of each cell\\n\\n![Slide modes](Slide%20modes.webp)\\nSlide modes\\n\\n* `Slide` - the cell will be a new slide\\n\\n* `Sub-Slide` - the cell will be shown in the current slide as a replacement for previous content. It will be available in arrow-down navigation\\n\\n* `Fragment` - the cell will appear in the current slide, and it will append to the previous content. It will be available in arrow-down and arrow-right navigation\\n\\n* `Skip` - the content will not be displayed in the presentation\\n\\n* `Notes` - notes for slide, the cell content is not displayed in the presentation\\n\\n_These explanations are taken from [here](https://mljar.com/blog/jupyter-notebook-presentation/)_\\n\\nAfter setting the required modes, we can export the .ipynb file to the presentation file using\\n\\n```bash\\njupyter nbconvert <notebook-name>.ipynb --to slides --post serve\\n```\\n* `--to slides` to convert the .ipynb file to a presentation file .html\\n\\n* `--post serve` to host the presentation file on the local HTTP server\\n\\nNow you could see something like this \u2193\\n\\n![Notebook Presentation](Notebook%20Presentation.gif)\\n\\nNotebook Presentation\\n\\nThat\'s all, thank you for reading the article, hope it helps to make your workflow faster."}]}}')}}]);